<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>RuglioBoi</title>
<link>https://ruglio.github.io/Web/</link>
<atom:link href="https://ruglio.github.io/Web/index.xml" rel="self" type="application/rss+xml"/>
<description>AR&#39;s personal website</description>
<generator>quarto-1.5.47</generator>
<lastBuildDate>Sun, 07 Jul 2024 22:00:00 GMT</lastBuildDate>
<item>
  <title>Multi-task Gaussian processes</title>
  <dc:creator>Andrea Ruglioni</dc:creator>
  <link>https://ruglio.github.io/Web/posts/dkl/s2/mgp.html</link>
  <description><![CDATA[ 





<p><a href="../../../posts/dkl/s1/gp.html">In my previous post on GPs</a>, we discussed the basics of GPs and their applications in regression tasks. Here, we extend the discussion to multi-task GPs, highlighting their benefits and practical implementations. We will provide an intuitive explanation of the concepts and showcase code examples using <code>GPyTorch</code>. Let’s dive in!</p>
<section id="understanding-multi-task-gps" class="level2">
<h2 class="anchored" data-anchor-id="understanding-multi-task-gps">Understanding multi-task GPs</h2>
<p>Gaussian processes are a powerful tool for regression and classification tasks, offering a non-parametric way to define distributions over functions. When dealing with multiple related outputs, a multi-task GP can model the dependencies between these tasks, leading to better generalization and predictions.</p>
<p>Mathematically, a Gaussian process is defined as a collection of random variables, any finite number of which have a joint Gaussian distribution. For a set of input points <img src="https://latex.codecogs.com/png.latex?X">, the corresponding output values <img src="https://latex.codecogs.com/png.latex?f(X)"> are jointly Gaussian:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Af(X)%20%5Csim%20%5Cmathcal%7BN%7D(m(X),%20k(X,%20X))%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?m(X)"> is the mean function and <img src="https://latex.codecogs.com/png.latex?k(X,%20X)"> is the covariance matrix.</p>
<p>In a multitask setting, we aim to model the function <img src="https://latex.codecogs.com/png.latex?f:%20%5Cmathcal%7BX%7D%20%5Cto%20%5Cmathbb%7BR%7D%5ET">, so that we have <img src="https://latex.codecogs.com/png.latex?T"> outputs, or tasks, <img src="https://latex.codecogs.com/png.latex?%5C%7Bf_t(X)%5C%7D_%7Bt=1%7D%5ET">. This means that the mean function is <img src="https://latex.codecogs.com/png.latex?m:%20%5Cmathcal%7BX%7D%20%5Cto%20%5Cmathbb%7BR%7D%5ET"> and the kernel function is <img src="https://latex.codecogs.com/png.latex?k:%20%5Cmathcal%7BX%7D%20%5Ctimes%20%5Cmathcal%7BX%7D%20%5Cto%20%5Cmathbb%7BR%7D%5E%7BT%20%5Ctimes%20T%7D">. How can we model the correlations between these tasks?</p>
<section id="independent-multi-task-gp" class="level3">
<h3 class="anchored" data-anchor-id="independent-multi-task-gp">Independent multi-task GP</h3>
<p>A simple independent multioutput GP models each task independently, without considering any correlations between tasks. In this setup, each task has its own GP with its own mean and covariance functions. Mathematically, this can be expressed as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Af_i(X)%20%5Csim%20%5Cmathcal%7BN%7D(m_i(X),%20k_i(X,%20X))%20%5Cqquad%20i%20=%201,%20%5Cldots,%20T,%0A"></p>
<p>leading to a block-diagonal covariance matrix <img src="https://latex.codecogs.com/png.latex?k(x,%20x)%20=%20%5Ctext%7Bdiag%7D(k_1(x,%20x),%20%5Cldots,%20k_T(x,%20x))">. This approach does not leverage any shared information between tasks, which can lead to suboptimal performance, especially when there is limited data for some tasks.</p>
</section>
<section id="intrinsic-model-of-coregionalization-icm" class="level3">
<h3 class="anchored" data-anchor-id="intrinsic-model-of-coregionalization-icm">Intrinsic model of coregionalization (ICM)</h3>
<p>The ICM approach generalizes the independent multioutput GP by introducing a coregionalization matrix <img src="https://latex.codecogs.com/png.latex?B"> that models the correlations between tasks. Specifically, the covariance function in the ICM approach is defined as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ak(x,%20x')%20=%20k_%7B%5Ctext%7Binput%7D%7D(x,%20x')%20B,%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?k_%7B%5Ctext%7Binput%7D%7D"> is a covariance function defined over the input space (e.g.&nbsp;squared exponential kernel), and <img src="https://latex.codecogs.com/png.latex?B%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BT%20%5Ctimes%20T%7D"> is the coregionalization matrix capturing the task-specific covariances. The matrix <img src="https://latex.codecogs.com/png.latex?B"> is typically parameterized as <img src="https://latex.codecogs.com/png.latex?B%20=%20W%20W%5E%5Cmathsf%7BT%7D">, with <img src="https://latex.codecogs.com/png.latex?W%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BT%20%5Ctimes%20r%7D"> and <img src="https://latex.codecogs.com/png.latex?r"> being the rank of the coregionalization matrix. This ensures the kernel is positive semi-definite.</p>
<p>The ICM approach can learn the shared structure between tasks. Indeed, the Pearson correlation coefficient between tasks can be expressed as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Crho_%7Bij%7D%20=%20%5Cfrac%7BB%5Bi,%20j%5D%7D%7B%5Csqrt%7BB%5Bi,%20i%5D%20B%5Bj,%20j%5D%7D%7D.%0A"></p>
</section>
<section id="linear-model-of-coregionalization-lmc" class="level3">
<h3 class="anchored" data-anchor-id="linear-model-of-coregionalization-lmc">Linear model of coregionalization (LMC)</h3>
<p>Another common approach is the LMC model, which extends the ICM by allowing for a wider variety of input kernels. In the LMC model, the covariance function is defined as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ak(x,%20x')%20=%20%5Csum_%7Bq=1%7D%5EQ%20k_%7B%5Ctext%7Binput%7D%7D%5E%7B(q)%7D(x,%20x')%20B_q,%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?Q"> is the number of base kernels, <img src="https://latex.codecogs.com/png.latex?k_%7B%5Ctext%7Binput%7D%7D%5E%7B(q)%7D"> are the base kernels, and <img src="https://latex.codecogs.com/png.latex?B_q"> are the coregionalization matrices for each base kernel. This model can capture even more complex correlations between tasks by combining multiple base kernels. We can recover the ICM model by setting <img src="https://latex.codecogs.com/png.latex?Q=1">.</p>
</section>
</section>
<section id="noise-modeling" class="level2">
<h2 class="anchored" data-anchor-id="noise-modeling">Noise modeling</h2>
<p>In multi-task GPs, we have to consider a multi-output likelihood function that models the noise for each task. The standard likelihood function is typically a multidimensional Gaussian likelihood, which can be expressed as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay%20=%20f(x)%20+%20%5Cepsilon,%20%5Cqquad%20%5Cepsilon%20%5Csim%20%5Cmathcal%7BN%7D_T(0,%20%5CSigma),%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?y"> is the observed output, <img src="https://latex.codecogs.com/png.latex?f(x)"> is the latent function, and <img src="https://latex.codecogs.com/png.latex?%5CSigma"> is the noise covariance matrix. The flexibility is on the choice of the noise covariance matrix, which can be diagonal <img src="https://latex.codecogs.com/png.latex?%5CSigma%20=%20%5Ctext%7Bdiag%7D(%5Csigma_1%5E2,%20%5Cldots,%20%5Csigma_T%5E2)"> (independent noise for each task) or full (correlated noise across tasks). The latter is usually represented as <img src="https://latex.codecogs.com/png.latex?%5CSigma%20=%20L%20L%5E%5Cmathsf%7BT%7D">, where <img src="https://latex.codecogs.com/png.latex?L%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BT%20%5Ctimes%20r%7D"> and <img src="https://latex.codecogs.com/png.latex?r"> is the rank of the noise covariance matrix. This allows for capturing correlations between the noise terms of different tasks.</p>
</section>
<section id="implementation-with-gpytorch" class="level2">
<h2 class="anchored" data-anchor-id="implementation-with-gpytorch">Implementation with <code>GPyTorch</code></h2>
<p>Let’s walk through an example of implementing a multitask GP using <code>GPyTorch</code> with the ICM kernel. First of all, we need to install the required packages, including <code>Torch</code>, <code>GPyTorch</code>, <code>matplotlib</code>, and <code>seaborn</code>, <code>numpy</code>.</p>
<div id="a87d5c86" class="cell" data-pip="true" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>pip install torch gpytorch matplotlib seaborn numpy pandas</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> gpytorch</span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> matplotlib <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-7"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> seaborn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sns</span>
<span id="cb1-8"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span></code></pre></div>
</details>
</div>
<p>Afterward, we can define the multitask GP model. We use an ICM kernel (with rank <img src="https://latex.codecogs.com/png.latex?r=1">) to capture correlations between tasks. We generate synthetic noisy training data for two tasks (sine and a shifted sine), so to have correlated outputs. The noise covariance matrix is</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5CSigma%20=%20%5Cbegin%7Bbmatrix%7D%0A%5Csigma_1%5E2%20&amp;%20%5Crho%20%5Csigma_1%20%5Csigma_2%20%5C%5C%0A%5Crho%20%5Csigma_1%20%5Csigma_2%20&amp;%20%5Csigma_2%5E2%0A%5Cend%7Bbmatrix%7D,%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Csigma_1%5E2%20=%20%5Csigma_2%5E2%20=%200.1%5E2"> and <img src="https://latex.codecogs.com/png.latex?%5Crho%20=%200.3">.</p>
<p>Lastly, we train the model and evaluate its performance by plotting the mean predictions and confidence intervals for each task.</p>
<div id="cell-fig-multitaskgpicm" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Define the kernel with coregionalization</span></span>
<span id="cb2-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> MultitaskGPModel(gpytorch.models.ExactGP):</span>
<span id="cb2-3">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, train_x, train_y, likelihood, num_tasks):</span>
<span id="cb2-4">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>(MultitaskGPModel, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>).<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(train_x, train_y, likelihood)</span>
<span id="cb2-5">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.mean_module <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> gpytorch.means.MultitaskMean(</span>
<span id="cb2-6">            gpytorch.means.ConstantMean(), num_tasks<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_tasks</span>
<span id="cb2-7">        )</span>
<span id="cb2-8">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.covar_module <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> gpytorch.kernels.MultitaskKernel(</span>
<span id="cb2-9">            gpytorch.kernels.RBFKernel(), num_tasks<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_tasks, rank<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb2-10">        )</span>
<span id="cb2-11"></span>
<span id="cb2-12">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x):</span>
<span id="cb2-13">        mean_x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.mean_module(x)</span>
<span id="cb2-14">        covar_x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.covar_module(x)</span>
<span id="cb2-15">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)</span>
<span id="cb2-16"></span>
<span id="cb2-17"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Training data</span></span>
<span id="cb2-18">f1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x:  torch.sin(x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> torch.pi))</span>
<span id="cb2-19">f2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: torch.sin((x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> torch.pi))</span>
<span id="cb2-20">train_x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.linspace(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb2-21">train_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.stack([</span>
<span id="cb2-22">    f1(train_x),</span>
<span id="cb2-23">    f2(train_x)</span>
<span id="cb2-24">]).T</span>
<span id="cb2-25"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Define the noise covariance matrix with correlation = 0.3</span></span>
<span id="cb2-26">sigma2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb2-27">Sigma <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor([[sigma2, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> sigma2], [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> sigma2, sigma2]])</span>
<span id="cb2-28"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Add noise to the training data</span></span>
<span id="cb2-29">train_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> torch.tensor(np.random.multivariate_normal(mean<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], cov<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>Sigma, size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(train_x)))</span>
<span id="cb2-30"></span>
<span id="cb2-31"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Model and likelihood</span></span>
<span id="cb2-32">num_tasks <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb2-33">likelihood <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_tasks, rank<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb2-34">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> MultitaskGPModel(train_x, train_y, likelihood, num_tasks)</span>
<span id="cb2-35"></span>
<span id="cb2-36"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Training the model</span></span>
<span id="cb2-37">model.train()</span>
<span id="cb2-38">likelihood.train()</span>
<span id="cb2-39"></span>
<span id="cb2-40">optimizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.optim.Adam(model.parameters(), lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>)</span>
<span id="cb2-41"></span>
<span id="cb2-42">mll <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)</span>
<span id="cb2-43"></span>
<span id="cb2-44">scheduler <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.optim.lr_scheduler.StepLR(optimizer, step_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, gamma<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>)</span>
<span id="cb2-45"></span>
<span id="cb2-46">num_iter <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span></span>
<span id="cb2-47"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(num_iter):</span>
<span id="cb2-48">    optimizer.zero_grad()</span>
<span id="cb2-49">    output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(train_x)</span>
<span id="cb2-50">    loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>mll(output, train_y)</span>
<span id="cb2-51">    loss.backward()</span>
<span id="cb2-52">    optimizer.step()</span>
<span id="cb2-53">    scheduler.step()</span>
<span id="cb2-54"></span>
<span id="cb2-55"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Evaluation</span></span>
<span id="cb2-56">model.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()</span>
<span id="cb2-57">likelihood.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()</span>
<span id="cb2-58"></span>
<span id="cb2-59">test_x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.linspace(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb2-60"></span>
<span id="cb2-61"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad(), gpytorch.settings.fast_pred_var():</span>
<span id="cb2-62">    pred_multi <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> likelihood(model(test_x))</span>
<span id="cb2-63"></span>
<span id="cb2-64"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Plot predictions</span></span>
<span id="cb2-65">fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots()</span>
<span id="cb2-66"></span>
<span id="cb2-67">colors <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'blue'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span>]</span>
<span id="cb2-68"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(num_tasks):</span>
<span id="cb2-69">    ax.plot(test_x, pred_multi.mean[:, i], label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Mean prediction (Task </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">)'</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>colors[i])</span>
<span id="cb2-70">    ax.plot(test_x, [f1(test_x), f2(test_x)][i], linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'True function (Task </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">)'</span>)</span>
<span id="cb2-71">    lower <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pred_multi.confidence_region()[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][:, i].detach().numpy()</span>
<span id="cb2-72">    upper <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pred_multi.confidence_region()[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>][:, i].detach().numpy()</span>
<span id="cb2-73">    ax.fill_between(</span>
<span id="cb2-74">        test_x,</span>
<span id="cb2-75">        lower,</span>
<span id="cb2-76">        upper,</span>
<span id="cb2-77">        alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>,</span>
<span id="cb2-78">        label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Confidence interval (Task </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">)'</span>,</span>
<span id="cb2-79">        color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>colors[i]</span>
<span id="cb2-80">    )</span>
<span id="cb2-81"></span>
<span id="cb2-82">ax.scatter(train_x, train_y[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'black'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Training data (Task 1)'</span>)</span>
<span id="cb2-83">ax.scatter(train_x, train_y[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'gray'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Training data (Task 2)'</span>)</span>
<span id="cb2-84"></span>
<span id="cb2-85">ax.set_title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Multitask GP with ICM'</span>)</span>
<span id="cb2-86">ax.legend(loc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lower center'</span>, bbox_to_anchor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>),</span>
<span id="cb2-87">          ncol<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, fancybox<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-multitaskgpicm" class="quarto-float quarto-figure quarto-figure-center anchored" width="709" height="476" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-multitaskgpicm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://ruglio.github.io/Web/posts/dkl/s2/mgp_files/figure-html/fig-multitaskgpicm-output-1.png" id="fig-multitaskgpicm" class="quarto-figure quarto-figure-center anchored figure-img" width="709" height="476">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-multitaskgpicm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
</figcaption>
</figure>
</div>
</div>
</div>
<p>Using <code>GPyTorch</code>, the ICM model is straightforward to implement by using the <code>MultitaskMean</code>, <code>MultitaskKernel</code>, and <code>MultitaskGaussianLikelihood</code> classes. These take care of the multitask structure, the noise and coregionalization matrices, allowing us to focus on the model definition and training.</p>
<p>Regarding the training loop, it works similarly to standard GPs, with the negative marginal log-likelihood as the loss function, and an optimizer to update the model parameters. A scheduler has been added to reduce the learning rate during training, which can help stabilize the optimization process.</p>
<p>Figure&nbsp;2 show the coregionalization matrix <img src="https://latex.codecogs.com/png.latex?B"> learned by the model, and the noise covariance matrix <img src="https://latex.codecogs.com/png.latex?%5CSigma">. The former captures the correlations between the tasks. As we can see, the off-diagonal elements of <img src="https://latex.codecogs.com/png.latex?B"> are positive. The latter represents the noise levels for each task. Notice that the model has properly learned the noise correlation.</p>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">W <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.covar_module.task_covar_module.covar_factor</span>
<span id="cb3-2">B <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> W <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> W.T</span>
<span id="cb3-3"></span>
<span id="cb3-4">fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots()</span>
<span id="cb3-5">sns.heatmap(B.detach().numpy(), annot<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax, cbar<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, square<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb3-6">ax.set_xticklabels([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Task 1'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Task 2'</span>])</span>
<span id="cb3-7">ax.set_yticklabels([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Task 1'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Task 2'</span>])</span>
<span id="cb3-8">ax.set_title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Coregionalization matrix B'</span>)</span>
<span id="cb3-9">fig.show()</span>
<span id="cb3-10"></span>
<span id="cb3-11"></span>
<span id="cb3-12">L <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.likelihood.task_noise_covar_factor.detach().numpy()</span>
<span id="cb3-13">Sigma <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> L <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> L.T</span>
<span id="cb3-14"></span>
<span id="cb3-15">fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots()</span>
<span id="cb3-16">sns.heatmap(Sigma, annot<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax, cbar<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, square<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb3-17">ax.set_xticklabels([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Task 1'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Task 2'</span>])</span>
<span id="cb3-18">ax.set_yticklabels([])</span>
<span id="cb3-19">ax.set_title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Noise covariance matrix'</span>)</span>
<span id="cb3-20">fig.show()</span></code></pre></div>
</details>
<div id="fig-matrices" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-matrices-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-matrices" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-matrices-1" class="quarto-float quarto-figure quarto-figure-center anchored" width="411" height="431" data-fig-align="center">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-matrices-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://ruglio.github.io/Web/posts/dkl/s2/mgp_files/figure-html/fig-matrices-output-1.png" id="fig-matrices-1" class="quarto-figure quarto-figure-center anchored figure-img" data-ref-parent="fig-matrices" width="411" height="431">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-matrices-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-matrices" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-matrices-2" class="quarto-float quarto-figure quarto-figure-center anchored" width="389" height="431" data-fig-align="center">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-matrices-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://ruglio.github.io/Web/posts/dkl/s2/mgp_files/figure-html/fig-matrices-output-2.png" id="fig-matrices-2" class="quarto-figure quarto-figure-center anchored figure-img" data-ref-parent="fig-matrices" width="389" height="431">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-matrices-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-matrices-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2
</figcaption>
</figure>
</div>
<section id="comparison" class="level3">
<h3 class="anchored" data-anchor-id="comparison">Comparison</h3>
<p>To highlight the advantages of modeling correlated outputs using the ICM approach, let’s compare it with a model that treats each task independently, ignoring any potential correlations between tasks. We can define a separate GP for each task, train them, and evaluate their performance on the test data.</p>
<div id="cell-fig-independentgps" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> IndependentGPModel(gpytorch.models.ExactGP):</span>
<span id="cb4-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, train_x, train_y, likelihood):</span>
<span id="cb4-3">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>(IndependentGPModel, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>).<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(train_x, train_y, likelihood)</span>
<span id="cb4-4">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.mean_module <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> gpytorch.means.ConstantMean()</span>
<span id="cb4-5">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.covar_module <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())</span>
<span id="cb4-6"></span>
<span id="cb4-7">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x):</span>
<span id="cb4-8">        mean_x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.mean_module(x)</span>
<span id="cb4-9">        covar_x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.covar_module(x)</span>
<span id="cb4-10">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> gpytorch.distributions.MultivariateNormal(mean_x, covar_x)</span>
<span id="cb4-11"></span>
<span id="cb4-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Create models and likelihoods for each task</span></span>
<span id="cb4-13">likelihoods <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [gpytorch.likelihoods.GaussianLikelihood() <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(num_tasks)]</span>
<span id="cb4-14">models <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [IndependentGPModel(train_x, train_y[:, i], likelihoods[i]) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(num_tasks)]</span>
<span id="cb4-15"></span>
<span id="cb4-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Training the independent models</span></span>
<span id="cb4-17"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, (model, likelihood) <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(models, likelihoods)):</span>
<span id="cb4-18">    model.train()</span>
<span id="cb4-19">    likelihood.train()</span>
<span id="cb4-20">    optimizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.optim.Adam(model.parameters(), lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>)</span>
<span id="cb4-21">    mll <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)</span>
<span id="cb4-22">    scheduler <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.optim.lr_scheduler.StepLR(optimizer, step_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, gamma<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>)</span>
<span id="cb4-23">    </span>
<span id="cb4-24">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(num_iter):</span>
<span id="cb4-25">        optimizer.zero_grad()</span>
<span id="cb4-26">        output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(train_x)</span>
<span id="cb4-27">        loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>mll(output, train_y[:, i])</span>
<span id="cb4-28">        loss.backward()</span>
<span id="cb4-29">        optimizer.step()</span>
<span id="cb4-30">        scheduler.step()</span>
<span id="cb4-31"></span>
<span id="cb4-32"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Evaluation</span></span>
<span id="cb4-33"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> model, likelihood <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(models, likelihoods):</span>
<span id="cb4-34">    model.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()</span>
<span id="cb4-35">    likelihood.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()</span>
<span id="cb4-36"></span>
<span id="cb4-37"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad(), gpytorch.settings.fast_pred_var():</span>
<span id="cb4-38">    pred_inde <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [likelihood(model(test_x)) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> model, likelihood <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(models, likelihoods)]</span>
<span id="cb4-39"></span>
<span id="cb4-40"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Plot predictions</span></span>
<span id="cb4-41">fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots()</span>
<span id="cb4-42"></span>
<span id="cb4-43"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(num_tasks):</span>
<span id="cb4-44">    ax.plot(test_x, pred_inde[i].mean, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Mean prediction (Task </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">)'</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>colors[i])</span>
<span id="cb4-45">    ax.plot(test_x, [f1(test_x), f2(test_x)][i], linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'True function (Task </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">)'</span>)</span>
<span id="cb4-46">    lower <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pred_inde[i].confidence_region()[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb4-47">    upper <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pred_inde[i].confidence_region()[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb4-48">    ax.fill_between(</span>
<span id="cb4-49">        test_x,</span>
<span id="cb4-50">        lower,</span>
<span id="cb4-51">        upper,</span>
<span id="cb4-52">        alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>,</span>
<span id="cb4-53">        label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Confidence interval (Task </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">)'</span>,</span>
<span id="cb4-54">        color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>colors[i]</span>
<span id="cb4-55">    )</span>
<span id="cb4-56"></span>
<span id="cb4-57">ax.scatter(train_x, train_y[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'black'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Training data (Task 1)'</span>)</span>
<span id="cb4-58">ax.scatter(train_x, train_y[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'gray'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Training data (Task 2)'</span>)</span>
<span id="cb4-59"></span>
<span id="cb4-60">ax.set_title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Independent GPs'</span>)</span>
<span id="cb4-61">ax.legend(loc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lower center'</span>, bbox_to_anchor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>),</span>
<span id="cb4-62">          ncol<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, fancybox<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-independentgps" class="quarto-float quarto-figure quarto-figure-center anchored" width="709" height="476" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-independentgps-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://ruglio.github.io/Web/posts/dkl/s2/mgp_files/figure-html/fig-independentgps-output-1.png" id="fig-independentgps" class="quarto-figure quarto-figure-center anchored figure-img" width="709" height="476">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-independentgps-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
</figcaption>
</figure>
</div>
</div>
</div>
<p>In terms of performance, we can compare the mean squared error (MSE) of the predictions on the test data for the multitask GP with ICM and the independent GPs.</p>
<div id="1349dc23" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">mean_multi <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pred_multi.mean.numpy()</span>
<span id="cb5-2">mean_inde <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.stack([pred.mean.numpy() <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> pred <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> pred_inde]).T</span>
<span id="cb5-3"></span>
<span id="cb5-4">test_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.stack([f1(test_x), f2(test_x)]).T.numpy()</span>
<span id="cb5-5">MSE_multi <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean((mean_multi <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> test_y) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb5-6">MSE_inde <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean((mean_inde <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> test_y) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb5-7"></span>
<span id="cb5-8">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame({</span>
<span id="cb5-9">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Model'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ICM'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Independent'</span>],</span>
<span id="cb5-10">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'MSE'</span>: [MSE_multi, MSE_inde]</span>
<span id="cb5-11">  })</span>
<span id="cb5-12">df</span></code></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="6">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Model</th>
<th data-quarto-table-cell-role="th">MSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>ICM</td>
<td>0.002097</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Independent</td>
<td>0.002626</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>The results show that ICM slightly outperforms the independent GPs in terms of MSE, thanks to the shared structure learned by the coregionalization matrix. In practice, the improvement can be more significant when dealing with more complex tasks or limited data. Indeed, in the independent scenario, each GP learns from a smaller dataset of 10 points, potentially leading to overfitting or suboptimal generalization. On the other hand, the multitask GP with ICM uses all the 20 points to learn the squared exponential kernel parameters. This shared information helps to improve the predictions for both tasks.</p>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-NIPS2007_66368270" class="csl-entry">
Bonilla, Edwin V, Kian Chai, and Christopher Williams. 2007. <span>“Multi-Task Gaussian Process Prediction.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by J. Platt, D. Koller, Y. Singer, and S. Roweis. Vol. 20. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2007/file/66368270ffd51418ec58bd793f2d9b1b-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2007/file/66368270ffd51418ec58bd793f2d9b1b-Paper.pdf</a>.
</div>
<div id="ref-FORRESTER200950" class="csl-entry">
Forrester, Alexander I. J., and Andy J. Keane. 2009. <span>“Recent Advances in Surrogate-Based Optimization.”</span> <em>Progress in Aerospace Sciences</em> 45 (1): 50–79. <a href="https://www.sciencedirect.com/science/article/pii/S0376042108000766">https://www.sciencedirect.com/science/article/pii/S0376042108000766</a>.
</div>
<div id="ref-frazier2018tutorial" class="csl-entry">
Frazier, Peter I. 2018. <span>“<span class="nocase">A Tutorial on Bayesian Optimization</span>.”</span> <a href="https://arxiv.org/abs/1807.02811">https://arxiv.org/abs/1807.02811</a>.
</div>
<div id="ref-garnett_bayesoptbook_2023" class="csl-entry">
Garnett, Roman. 2023. <em><span>Bayesian Optimization</span></em>. Cambridge University Press.
</div>
</div></section></div> ]]></description>
  <category>machine-learning</category>
  <guid>https://ruglio.github.io/Web/posts/dkl/s2/mgp.html</guid>
  <pubDate>Sun, 07 Jul 2024 22:00:00 GMT</pubDate>
  <media:content url="https://ruglio.github.io/Web/posts/dkl/s2/mgp.png" medium="image" type="image/png" height="107" width="144"/>
</item>
<item>
  <title>Introduction to Gaussian processes</title>
  <dc:creator>Andrea Ruglioni</dc:creator>
  <link>https://ruglio.github.io/Web/posts/dkl/s1/gp.html</link>
  <description><![CDATA[ 





<p>Welcome to the first installment of our series on deep kernel learning. In this post, we’ll delve into Gaussian processes (GPs) and their application as regressors. We’ll start by exploring what GPs are and why they are powerful tools for regression tasks. In subsequent posts, we’ll build on this foundation to discuss multi-task Gaussian processes and how they can be combined with neural networks to create deep kernel models.</p>
<section id="gaussian-processes" class="level2">
<h2 class="anchored" data-anchor-id="gaussian-processes">Gaussian processes</h2>
<p>To understand Gaussian processes fully, it’s important to briefly mention the Kolmogorov extension theorem. This theorem guarantees the existence of a stochastic process, i.e., a collection of random variables <img src="https://latex.codecogs.com/png.latex?%5C%7BY_x%5C%7D_%7Bx%20%5Cin%20%5Cmathcal%7BX%7D%7D,%20Y_x%20%5Cin%20%5Cmathbb%7BR%7D">, that satisfies a specified finite-dimensional distribution. For instance, it ensures that we can define a Gaussian process by specyfing that any finite set of random variables has a multivariate Gaussian distribution, without worrying about the infinite-dimensional nature of the process. Observe that, in a similar matter, we could define a t-student process, by imposing that finite-dimensional distributions are t-student.</p>
<p>Therefore, similar to a multivariate Gaussian distribution, a Gaussian process <img src="https://latex.codecogs.com/png.latex?f"> is defined by its mean function <img src="https://latex.codecogs.com/png.latex?m(%5Ccdot)%20:%20%5Cmathcal%7BX%7D%20%5Cto%20%5Cmathbb%7BR%7D"> and covariance function <img src="https://latex.codecogs.com/png.latex?k(%5Ccdot,%20%5Ccdot)%20:%20%5Cmathcal%7BX%7D%20%5Ctimes%20%5Cmathcal%7BX%7D%20%5Cto%20%5Cmathbb%7BR%7D">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Af%20%5Csim%20GP(m,%20k),%0A"></p>
<p>and it can be interpreted as an infinite-dimensional generalization of a multivariate Gaussian distribution.</p>
<p>In a regression setting, we could use <img src="https://latex.codecogs.com/png.latex?f"> as a surrogate model of a function <img src="https://latex.codecogs.com/png.latex?g:%20%5Cmathcal%7BX%7D%20%5Cto%20%5Cmathbb%7BR%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BX%7D"> is the input space. Suppose you have a set of input points <img src="https://latex.codecogs.com/png.latex?X%20=%20%5C%7Bx_1,%20x_2,%20%5Cldots,%20x_n%5C%7D">, with observations <img src="https://latex.codecogs.com/png.latex?Y%20=%20%5C%7By_1%20=%20g(x_1),%20y_2%20=%20g(x_2),%20%5Cldots,%20y_n%20=%20g(x_n)%5C%7D">, the joint distribution of the observed outputs <img src="https://latex.codecogs.com/png.latex?Y">, assuming a GP prior, is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bpmatrix%7D%0Ay_1%20%5C%5C%0A%5Cvdots%20%5C%5C%0Ay_n%0A%5Cend%7Bpmatrix%7D%20%5Csim%20%5Cmathcal%7BN%7D%5Cleft(%0A%5Cmathbf%7Bm%7D%20=%0A%5Cbegin%7Bpmatrix%7D%0Am(x_1)%20%5C%5C%0A%5Cvdots%20%5C%5C%0Am(x_n)%0A%5Cend%7Bpmatrix%7D,%0A%5Cmathbf%7BK%7D%20=%0A%5Cbegin%7Bpmatrix%7D%0Ak(x_1,%20x_1)%20&amp;%20%5Cdots%20&amp;%20k(x_1,%20x_n)%20%5C%5C%0A%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%0Ak(x_n,%20x_1)%20&amp;%20%5Cdots%20&amp;%20k(x_n,%20x_n)%0A%5Cend%7Bpmatrix%7D%5Cright),%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bm%7D"> is the vector of mean function values and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BK%7D"> is the covariance matrix of the function values at the input points. This approach allows us to make predictions at new input points <img src="https://latex.codecogs.com/png.latex?x_*"> by conditioning on the observed data, providing not only point estimates but also uncertainty estimates.</p>
<section id="making-predictions" class="level3">
<h3 class="anchored" data-anchor-id="making-predictions">Making predictions</h3>
<p>To make a prediction <img src="https://latex.codecogs.com/png.latex?y_*%20=%20g(x_*)"> at new input point, we use the joint distribution of the observed outputs <img src="https://latex.codecogs.com/png.latex?Y"> and the function values at <img src="https://latex.codecogs.com/png.latex?x_*">, which is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bpmatrix%7D%0AY%20%5C%5C%0Ay_*%0A%5Cend%7Bpmatrix%7D%20%5Csim%20%5Cmathcal%7BN%7D%5Cleft(%0A%5Cbegin%7Bpmatrix%7D%0A%5Cmathbf%7Bm%7D%20%5C%5C%0Am(x_*)%0A%5Cend%7Bpmatrix%7D,%0A%5Cbegin%7Bpmatrix%7D%0A%5Cmathbf%7BK%7D%20&amp;%20k(X,%20x_*)%20%5C%5C%0Ak(x_*,%20X)%20&amp;%20k(x_*,%20x_*)%0A%5Cend%7Bpmatrix%7D%5Cright)%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?k(x_*,%20X)"> is vector of covariances between the new input point <img src="https://latex.codecogs.com/png.latex?x_*"> and the observed data points <img src="https://latex.codecogs.com/png.latex?X">. The conditional distribution of <img src="https://latex.codecogs.com/png.latex?y_*"> given <img src="https://latex.codecogs.com/png.latex?Y"> is then Gaussian with mean and covariance:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmu(x_*%20%5Cmid%20X,%20Y)%20=%20k(x_*,%20X)%20%5Cmathbf%7BK%7D%5E%7B-1%7D%20(Y%20-%20%5Cmathbf%7Bm%7D),%0A"> <img src="https://latex.codecogs.com/png.latex?%0As%5E2(x_*%20%5Cmid%20X,%20Y)%20=%20k(x_*,%20x_*)%20-%20k(x_*,%20X)%20%5Cmathbf%7BK%7D%5E%7B-1%7D%20k(X,%20x_*).%0A"></p>
<p>Therefore, given the observed data, we can estimate the function value at a new input point <img src="https://latex.codecogs.com/png.latex?x_*"> as <img src="https://latex.codecogs.com/png.latex?%5Cmu(x_*)"> and quantify the uncertainty in the prediction as <img src="https://latex.codecogs.com/png.latex?s%5E2(x_*)">. This is a key advantage of GPs, which can be important in decision-making processes.</p>
</section>
</section>
<section id="interactive-visualizations" class="level2">
<h2 class="anchored" data-anchor-id="interactive-visualizations">Interactive visualizations</h2>
<p>Let’s explore some interactive plots to better understand how the kernel functions influence the Gaussian process model. Indeed, the choice of the kernel function is crucial in defining the prior over functions, as it determines the smoothness and periodicity of the functions that the GP can model. Therefore, they play a fundamental role in the model’s flexibility and generalization capabilities, and they can be tailored to the specific characteristics of the data at hand. On the other hand, the mean function is usually set constant, as the kernel is flexible enough.</p>
<section id="squared-exponential-kernel" class="level3">
<h3 class="anchored" data-anchor-id="squared-exponential-kernel">Squared exponential kernel</h3>
<p>The squared exponential kernel (also known as the RBF kernel) is defined as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ak_%7B%5Ctext%7BExp%7D%7D(x,%20x')%20=%20%5Csigma%5E2%20%5Cexp%20%5Cleft(%20-%5Cfrac%7B(x%20-%20x')%5E2%7D%7B2l%5E2%7D%20%5Cright)%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2"> is the variance and <img src="https://latex.codecogs.com/png.latex?l"> is the length scale. Below is an interactive plot that shows how the squared exponential kernel depends on the lengthscale and variance. Notice that with a small length scale, the function is more wiggly. Instead, with a large length scale it is smoother, as the kernel function decays more slowly with distance (i.e., the correlation between faraway points is higher, and they are more similar to each other). Instead, the variance controls the amplitude of the function, with higher values leading to more variability.</p>
<pre class="shinylive-python" data-engine="python"><code>#| standalone: true
#| viewerHeight: 475

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal
from shiny import App, render, ui


# Define the kernel function
def exponential_quadratic_kernel(x1, x2, l=1.0, sigma2=1.0):
    """Computes the exponential quadratic kernel (squared exponential kernel)."""
    sqdist = np.sum(x1**2, 1).reshape(-1, 1) + np.sum(x2**2, 1) - 2 * np.dot(x1, x2.T)
    return sigma2 * np.exp(-0.5 / l**2 * sqdist)

# Define the input space
X = np.linspace(-4, 4, 100).reshape(-1, 1)

def plot_kernel_and_samples(l, sigma2):
    # Compute the kernel matrix
    K = exponential_quadratic_kernel(X, X, l=l, sigma2=sigma2)

    # Create the plot
    fig, ax = plt.subplots(2, 1, figsize=(18, 6), sharex=True)

    # Plot the kernel
    ax[0].plot(X, exponential_quadratic_kernel(X, np.zeros((1,1)), l=l, sigma2=sigma2))
    ax[0].set_title(f"Squared exponential kernel function")

    # Sample 5 functions from the Gaussian process defined by the kernel
    mean = np.zeros(100)
    cov = K
    samples = multivariate_normal.rvs(mean, cov, 5)

    # Plot the samples
    for i in range(5):
        ax[1].plot(X, samples[i])
    ax[1].set_title("Samples from the GP")
    ax[1].set_xlabel("x")

    plt.tight_layout()
    return fig

app_ui = ui.page_fluid(
    ui.layout_sidebar(
        ui.panel_sidebar(
            ui.input_slider("length_scale", "Length Scale (l):", min=0.1, max=5.0, value=1.0, step=0.1),
            ui.input_slider("variance", "Variance (σ²):", min=0.1, max=5.0, value=1.0, step=0.1)
        ),
        ui.panel_main(
            ui.output_plot("kernelPlot")
        )
    )
)

def server(input, output, session):
    @output
    @render.plot
    def kernelPlot():
        l = input.length_scale()
        sigma2 = input.variance()
        fig = plot_kernel_and_samples(l, sigma2)
        return fig

app = App(app_ui, server)</code></pre>
</section>
<section id="matérn-kernel" class="level3">
<h3 class="anchored" data-anchor-id="matérn-kernel">Matérn kernel</h3>
<p>The Matérn kernel is defined as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ak_%7B%5Ctext%7BMat%C3%A9rn%7D%7D(x,%20x')%20=%20%5Csigma%5E2%20%5Cfrac%7B2%5E%7B1-%5Cnu%7D%7D%7B%5CGamma(%5Cnu)%7D%20%5Cleft(%20%5Csqrt%7B2%5Cnu%7D%20%5Cfrac%7B%7Cx%20-%20x'%7C%7D%7Bl%7D%20%5Cright)%5E%7B%5Cnu%7D%20K_%7B%5Cnu%7D%20%5Cleft(%20%5Csqrt%7B2%5Cnu%7D%20%5Cfrac%7B%7Cx%20-%20x'%7C%7D%7Bl%7D%20%5Cright)%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cnu"> controls the smoothness of the function, <img src="https://latex.codecogs.com/png.latex?l"> is the length scale, <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2"> is the variance, and <img src="https://latex.codecogs.com/png.latex?K_%7B%5Cnu%7D"> is the modified Bessel function. The former two parameters have the same effect as in the squared exponential kernel, while <img src="https://latex.codecogs.com/png.latex?%5Cnu"> controls the smoothness of the function. Indeed, we have that the samples generated have smoothness <img src="https://latex.codecogs.com/png.latex?%5Clceil%20%5Cnu%20%5Crceil%20-%201">, and for <img src="https://latex.codecogs.com/png.latex?%5Cnu%20%5Cto%20%5Cinfty">, the Matérn kernel converges to the squared exponential kernel, leading to infinitely smooth functions.</p>
<pre class="shinylive-python" data-engine="python"><code>#| standalone: true
#| viewerHeight: 475

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal
from shiny import App, render, ui
from scipy.special import kv, gamma
from scipy.spatial.distance import cdist


# Define the kernel function
def matern_kernel(x1, x2, l=1.0, sigma2=1.0, nu=1.5):
    """Computes the Matérn kernel."""
    D = cdist(x1, x2, 'euclidean')
    const = (2**(1-nu))/gamma(nu)
    K = const * (np.sqrt(2*nu)*D/l)**nu * kv(nu, np.sqrt(2*nu)*D/l)
    # Replace NaN values with 1 for x == x'
    K[np.isnan(K)] = 1
    K *= sigma2
    return K

# Define the input space
X = np.linspace(-4, 4, 100).reshape(-1, 1)

def plot_kernel_and_samples(l, sigma2, nu):
    # Compute the kernel matrix
    K = matern_kernel(X, X, l=l, sigma2=sigma2, nu=nu)

    # Create the plot
    fig, ax = plt.subplots(2, 1, figsize=(18, 6), sharex=True)

    # Plot the kernel
    ax[0].plot(X, matern_kernel(X, np.zeros((1, 1)), l=l, sigma2=sigma2, nu=nu))
    ax[0].set_title(f"Matern kernel function")

    # Sample 5 functions from the Gaussian process defined by the kernel
    mean = np.zeros(100)
    cov = K
    samples = multivariate_normal.rvs(mean, cov, 5)

    # Plot the samples
    for i in range(5):
        ax[1].plot(X, samples[i])
    ax[1].set_title("Samples from the GP")
    ax[1].set_xlabel("x")

    plt.tight_layout()
    return fig

app_ui = ui.page_fluid(
    ui.layout_sidebar(
        ui.panel_sidebar(
            ui.input_slider("length_scale", "Length Scale (l):", min=0.1, max=5.0, value=1.0, step=0.1),
            ui.input_slider("variance", "Variance (σ²):", min=0.1, max=5.0, value=1.0, step=0.1),
            ui.input_slider("smoothness_param", "Smoothness param (v):", min=1.5, max=5.0, value=1.5, step=0.5)
        ),
        ui.panel_main(
            ui.output_plot("kernelPlot")
        )
    )
)

def server(input, output, session):
    @output
    @render.plot
    def kernelPlot():
        l = input.length_scale()
        sigma2 = input.variance()
        nu = input.smoothness_param()
        fig = plot_kernel_and_samples(l, sigma2, nu)
        return fig

app = App(app_ui, server)</code></pre>
</section>
</section>
<section id="noisy-observations" class="level2">
<h2 class="anchored" data-anchor-id="noisy-observations">Noisy observations</h2>
<p>Noise and measurement error are inevitable in real-world data, which can significantly impact the performance and reliability of predictive models. As a result, it is essential to take noise into account when modeling data with GP. We can represent noisy observations as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay%20=%20g(x)%20+%20%5Cepsilon%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cepsilon"> is a random variable representing the noise. Usually, we assume that <img src="https://latex.codecogs.com/png.latex?%5Cepsilon%20%5Csim%20%5Cmathcal%7BN%7D(0,%20%5Csigma_n%5E2)">, where <img src="https://latex.codecogs.com/png.latex?%5Csigma_n%5E2"> is the variance of the noise. In this way, it can be easily incorporated into the GP by adding a diagonal noise term to the kernel matrix:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BK%7D_n%20=%20%5Cmathbf%7BK%7D%20+%20%5Csigma_n%5E2%20%5Cmathbf%7BI%7D,%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BK%7D"> is the kernel matrix computed on the training data and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BI%7D"> is the identity matrix.</p>
<p>Below is an interactive plot that demonstrates how noise influences the GP model. The plot shows the noisy training data (black points) of the true function <img src="https://latex.codecogs.com/png.latex?g(x)%20=%20%5Csin(x)">, the red dashed line. The plot also shows the GP mean prediction (blue line) for the squared exponential and Matérn kernels, along with the 95% confidence intervals. For <img src="https://latex.codecogs.com/png.latex?%5Csigma_n%5E2%20=%200">, the model perfectly interpolates the training data. For higher noise levels, the model becomes less certain about the observations, leading to a non-interpolating behavior, and the confidence intervals widen.</p>
<pre class="shinylive-python" data-engine="python"><code>#| standalone: true
#| viewerHeight: 475

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal
from shiny import App, render, ui
from scipy.spatial.distance import cdist
from scipy.special import kv, gamma

# Define the kernel functions
def exponential_quadratic_kernel(x1, x2, l=1.0, sigma2=1.0):
    """Computes the exponential quadratic kernel (squared exponential kernel)."""
    sqdist = np.sum(x1**2, 1).reshape(-1, 1) + np.sum(x2**2, 1) - 2 * np.dot(x1, x2.T)
    return sigma2 * np.exp(-0.5 / l**2 * sqdist)

def matern_kernel(x1, x2, l=1.0, sigma2=1.0, nu=1.5):
    """Computes the Matérn kernel."""
    D = cdist(x1, x2, 'euclidean')
    const = (2**(1-nu))/gamma(nu)
    K = sigma2 * const * (np.sqrt(2*nu)*D/l)**nu * kv(nu, np.sqrt(2*nu)*D/l)
    # Replace NaN values with 1 for x == x'
    K[np.isnan(K)] = 1
    return K

# Define the input space
X = np.linspace(-4, 4, 100).reshape(-1, 1)

def plot_kernels_and_samples(noise_level):
    # Generate training data with noise
    X_train = np.array([-3, -2, -1, 1, 3.5]).reshape(-1, 1)
    y_train = np.sin(X_train) + noise_level * np.random.randn(X_train.shape[0], 1).reshape(-1,1)

    # Compute the kernel matrices for training data
    K_train_exp = exponential_quadratic_kernel(X_train, X_train) + noise_level**2 * np.eye(len(X_train))
    K_s_exp = exponential_quadratic_kernel(X_train, X)
    K_ss_exp = exponential_quadratic_kernel(X, X)
    
    K_train_matern = matern_kernel(X_train, X_train) + noise_level**2 * np.eye(len(X_train))
    K_s_matern = matern_kernel(X_train, X)
    K_ss_matern = matern_kernel(X, X)

    # Compute the mean and covariance of the posterior distribution for exponential kernel
    K_train_inv_exp = np.linalg.inv(K_train_exp)
    mu_s_exp = (K_s_exp.T.dot(K_train_inv_exp).dot(y_train)).reshape(-1)
    cov_s_exp = K_ss_exp - K_s_exp.T.dot(K_train_inv_exp).dot(K_s_exp)

    # Compute the mean and covariance of the posterior distribution for Matérn kernel
    K_train_inv_matern = np.linalg.inv(K_train_matern)
    mu_s_matern = (K_s_matern.T.dot(K_train_inv_matern).dot(y_train)).reshape(-1)
    cov_s_matern = K_ss_matern - K_s_matern.T.dot(K_train_inv_matern).dot(K_s_matern)

    # Sample 5 functions from the posterior distribution for both kernels
    samples_exp = multivariate_normal.rvs(mu_s_exp, cov_s_exp, 5)
    samples_matern = multivariate_normal.rvs(mu_s_matern, cov_s_matern, 5)

    # Create the plot
    fig, ax = plt.subplots(2, 1, figsize=(18, 6), sharex=True)

    # Plot the training data and GP predictions for exponential kernel
    ax[0].scatter(X_train, y_train, color='black', zorder=10, label='Noisy observations')
    ax[0].plot(X, mu_s_exp, color='blue', label='Mean prediction')
    ax[0].plot(X, np.sin(X), color='r', linestyle='--', label='True function')
    ax[0].fill_between(X.ravel(), mu_s_exp - 1.96 * np.sqrt(np.diag(cov_s_exp)), mu_s_exp + 1.96 * np.sqrt(np.diag(cov_s_exp)), color="blue", alpha=0.2, label='Confidence interval')
    # for i in range(5):
    #     ax[0].plot(X, samples_exp[i], alpha=0.5, linestyle='--')
    ax[0].set_title(f"Squared exponential kernel")
    # ax[0].legend()

    # Plot the training data and GP predictions for Matérn kernel
    ax[1].scatter(X_train, y_train, color='black', zorder=10, label='Noisy observations')
    ax[1].plot(X, mu_s_matern, color="blue", label='Mean prediction')
    ax[1].plot(X, np.sin(X), color="r", linestyle='--', label='True function')
    ax[1].fill_between(X.ravel(), mu_s_matern - 1.96 * np.sqrt(np.diag(cov_s_matern)), mu_s_matern + 1.96 * np.sqrt(np.diag(cov_s_matern)), color="blue", alpha=0.2, label='Confidence interval')
    # for i in range(5):
    #     ax[1].plot(X, samples_matern[i], alpha=0.5, linestyle='--')
    ax[1].set_title(f"Matérn kernel")
    # ax[1].legend()
    ax[1].set_xlabel("x")

    plt.tight_layout()
    return fig

app_ui = ui.page_fluid(
    ui.layout_sidebar(
        ui.panel_sidebar(
            ui.input_slider("noise_level", "Noise Level (σ²ₙ):", min=0.0, max=1.0, value=0.0, step=0.01)
        ),
        ui.panel_main(
            ui.output_plot("kernelPlot")
        )
    )
)

def server(input, output, session):
    @output
    @render.plot
    def kernelPlot():
        noise_level = input.noise_level()
        fig = plot_kernels_and_samples(noise_level)
        return fig

app = App(app_ui, server)</code></pre>
</section>
<section id="code" class="level2">
<h2 class="anchored" data-anchor-id="code">Code</h2>
<p>In this section, we’ll provide a brief overview of how to implement Gaussian processes in Python using the <code>GPyTorch</code> library. The code snippet below demonstrates how to define a GP model with a squared exponential kernel and train it on synthetic data.</p>
<div id="c99c7fce" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb4-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb4-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb4-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> gpytorch</span>
<span id="cb4-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> gpytorch.kernels <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> RBFKernel, ScaleKernel</span>
<span id="cb4-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> gpytorch.means <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ConstantMean</span>
<span id="cb4-7"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> gpytorch.likelihoods <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> GaussianLikelihood</span>
<span id="cb4-8"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> gpytorch.models <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ExactGP</span>
<span id="cb4-9"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> gpytorch.mlls <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ExactMarginalLogLikelihood</span>
<span id="cb4-10"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> gpytorch.distributions <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> MultivariateNormal</span>
<span id="cb4-11"></span>
<span id="cb4-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Define the GP model</span></span>
<span id="cb4-13"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> GP(ExactGP):</span>
<span id="cb4-14">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, train_x, train_y, likelihood):</span>
<span id="cb4-15">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>(GP, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>).<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(train_x, train_y, likelihood)</span>
<span id="cb4-16">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.mean_module <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ConstantMean()</span>
<span id="cb4-17">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.covar_module <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ScaleKernel(RBFKernel())</span>
<span id="cb4-18"></span>
<span id="cb4-19">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x):</span>
<span id="cb4-20">        mean_x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.mean_module(x)</span>
<span id="cb4-21">        covar_x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.covar_module(x)</span>
<span id="cb4-22">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> MultivariateNormal(mean_x, covar_x)</span>
<span id="cb4-23"></span>
<span id="cb4-24"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Generate synthetic data</span></span>
<span id="cb4-25">f <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: torch.sin(x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.pi))</span>
<span id="cb4-26">train_x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.linspace(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb4-27">train_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> f(train_x) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> torch.randn(train_x.size()) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span></span>
<span id="cb4-28">likelihood <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> GaussianLikelihood()</span>
<span id="cb4-29"></span>
<span id="cb4-30"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Initialize the model and likelihood</span></span>
<span id="cb4-31">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> GP(train_x, train_y, likelihood)</span>
<span id="cb4-32"></span>
<span id="cb4-33"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Training the model</span></span>
<span id="cb4-34">model.train()</span>
<span id="cb4-35">likelihood.train()</span>
<span id="cb4-36"></span>
<span id="cb4-37"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Use the adam optimizer</span></span>
<span id="cb4-38">optimizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.optim.Adam(model.parameters(), lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>)</span>
<span id="cb4-39"></span>
<span id="cb4-40"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># "Loss" for GPs - the marginal log likelihood</span></span>
<span id="cb4-41">mll <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ExactMarginalLogLikelihood(likelihood, model)</span>
<span id="cb4-42"></span>
<span id="cb4-43"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Training loop</span></span>
<span id="cb4-44">training_iterations <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span></span>
<span id="cb4-45"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(training_iterations):</span>
<span id="cb4-46">    optimizer.zero_grad()</span>
<span id="cb4-47">    output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(train_x)</span>
<span id="cb4-48">    loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>mll(output, train_y)</span>
<span id="cb4-49">    loss.backward()</span>
<span id="cb4-50">    optimizer.step()</span>
<span id="cb4-51"></span>
<span id="cb4-52"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Set the model and likelihood into evaluation mode</span></span>
<span id="cb4-53">model.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()</span>
<span id="cb4-54">likelihood.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()</span>
<span id="cb4-55"></span>
<span id="cb4-56"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Make predictions</span></span>
<span id="cb4-57">test_x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.linspace(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb4-58"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad(), gpytorch.settings.fast_pred_var():</span>
<span id="cb4-59">    observed_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> likelihood(model(test_x))</span>
<span id="cb4-60">    mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> observed_pred.mean</span>
<span id="cb4-61">    lower, upper <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> observed_pred.confidence_region()</span>
<span id="cb4-62"></span>
<span id="cb4-63"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Plot the results</span></span>
<span id="cb4-64">fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots()</span>
<span id="cb4-65">ax.scatter(train_x, train_y, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'black'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Training data'</span>)</span>
<span id="cb4-66">ax.plot(test_x, f(test_x), <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'r--'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'True function'</span>)</span>
<span id="cb4-67">ax.plot(test_x, mean, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Mean prediction'</span>)</span>
<span id="cb4-68">ax.fill_between(test_x, lower, upper, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'blue'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Confidence interval'</span>)</span>
<span id="cb4-69">ax.set_xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'x'</span>)</span>
<span id="cb4-70">ax.set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'y'</span>)</span>
<span id="cb4-71">ax.legend()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://ruglio.github.io/Web/posts/dkl/s1/gp_files/figure-html/cell-2-output-1.png" class="quarto-figure quarto-figure-center figure-img" width="600" height="429"></p>
</figure>
</div>
</div>
</div>
<p>The three main components that need to be defined are the mean function, the kernel function, and the likelihood function (which models the noise in the data). Observe that the base kernel is the <code>RBFKernel</code>, which corresponds to the squared exponential kernel, and it is wrapped by the <code>ScaleKernel</code> to allow for the scaling of the kernel through the variance parameter <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2">. The <code>ExactMarginalLogLikelihood</code> object is used to compute the marginal log likelihood, which is the negative loss function for training the GP model. Indeed, the GP model parameters are optimized by maximizing the marginal log likelihood of the observed data, which is given by</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D(%5Ctheta)%20=%20%5Clog%20p(Y%20%5Cmid%20X,%20%5Ctheta)%0A%20%20%20%20=%20-%5Cfrac%7B1%7D%7B2%7D%20(Y%20-%20%5Cmathbf%7Bm%7D)%5E%5Cmathsf%7BT%7D%20%5Cmathbf%7BK%7D_n%5E%7B-1%7D%20(Y%20-%20%5Cmathbf%7Bm%7D)%20-%20%5Cfrac%7B1%7D%7B2%7D%20%5Clog%20%7C%5Cmathbf%7BK%7D_n%7C%20-%20%5Cfrac%7BN%7D%7B2%7D%20%5Clog(2%5Cpi),%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> are the model parameters, comprising the mean, kernel, and likelihood parameters. Computaionally speaking, the inversion of the kernel matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BK%7D_n"> is the most expensive operation, with a complexity of <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D(N%5E3)">, where <img src="https://latex.codecogs.com/png.latex?N"> is the number of training points. Therefore, for very large datasets, approximate inference methods, inducing points, or sparse GPs should be used to reduce the computational burden.</p>
<p>Lastly, observe that the <code>ExactGP</code> class is the standard GP for Gaussian likelihoods, where the exact marginal log likelihood can be computed in closed form. However, <code>GPyTorch</code> also provides different likelihoods, such as student-t likelihoods (which is more stable if outliers are present) and more. In these cases, the must class <code>ApproximateGP</code> should be used, which allows for approximate inference methods like variational inference. Regarding the loss function, the <code>ExactMarginalLogLikelihood</code> should be replaced by the <code>VariationalELBO</code> object, or other appropriate loss functions for approximate inference.</p>
</section>
<section id="applications" class="level2">
<h2 class="anchored" data-anchor-id="applications">Applications</h2>
<section id="bayesian-hyperparameter-tuning" class="level3">
<h3 class="anchored" data-anchor-id="bayesian-hyperparameter-tuning">Bayesian hyperparameter tuning</h3>
<p>Hyperparameter tuning is a critical yet challenging aspect of training neural networks. Finding the optimal combination of hyperparameters, such as learning rate, batch size, number of layers, and units per layer, can significantly enhance a model’s performance. Traditional methods like grid search and random search often prove to be inefficient and computationally expensive. This is where Bayesian optimization, powered by GPs, comes into play, offering a smarter approach to hyperparameter tuning.</p>
<p>Unlike exhaustive search methods, Bayesian optimization is more sample-efficient, meaning it can find optimal hyperparameters with fewer iterations. It works by</p>
<ol type="1">
<li>modeling the objective function (e.g., validation loss) as a GP in the hyperparameter space.</li>
<li>using an acquisition function to decide where to sample next. The acquisition function balances exploration (sampling in unexplored regions) and exploitation (sampling in regions with low loss) to guide the search towards the global optimum.</li>
</ol>
</section>
<section id="time-series-forecasting" class="level3">
<h3 class="anchored" data-anchor-id="time-series-forecasting">Time series forecasting</h3>
<p>GPs are also widely used in time series forecasting due to their flexibility and ability to model complex patterns in the data. By treating time series data as a function of time, Gaussian processes can capture the underlying dynamics and dependencies in the series. They can provide not only point estimates but also probabilistic forecasts, including prediction intervals that quantify uncertainty.</p>
</section>
</section>
<section id="references" class="level2">




</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-FORRESTER200950" class="csl-entry">
Forrester, Alexander I. J., and Andy J. Keane. 2009. <span>“Recent Advances in Surrogate-Based Optimization.”</span> <em>Progress in Aerospace Sciences</em> 45 (1): 50–79. <a href="https://www.sciencedirect.com/science/article/pii/S0376042108000766">https://www.sciencedirect.com/science/article/pii/S0376042108000766</a>.
</div>
<div id="ref-frazier2018tutorial" class="csl-entry">
Frazier, Peter I. 2018. <span>“<span class="nocase">A Tutorial on Bayesian Optimization</span>.”</span> <a href="https://arxiv.org/abs/1807.02811">https://arxiv.org/abs/1807.02811</a>.
</div>
<div id="ref-garnett_bayesoptbook_2023" class="csl-entry">
Garnett, Roman. 2023. <em><span>Bayesian Optimization</span></em>. Cambridge University Press.
</div>
</div></section></div> ]]></description>
  <category>machine-learning</category>
  <guid>https://ruglio.github.io/Web/posts/dkl/s1/gp.html</guid>
  <pubDate>Mon, 24 Jun 2024 22:00:00 GMT</pubDate>
  <media:content url="https://ruglio.github.io/Web/posts/dkl/s1/gp.png" medium="image" type="image/png" height="105" width="144"/>
</item>
</channel>
</rss>
