[
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Andrea Ruglioni",
    "section": "",
    "text": "EPFL Fall 2024, TA as PhD student\nThis course gives an introduction to probability theory and stochastic calculus in discrete and continuous time. The fundamental notions and techniques introduced in this course have many applications in finance, for example for option pricing, risk management and optimal portfolio choice.\n\n\n\nKTH Royal Institute of Technology Fall 2023, TA as graduate student\nThe course contents will mainly be based on the following areas:\n\nQueueing models based on Markov processes, including models for queueing networks.\nModels for inventory optimization, deterministic as well as stochastic.\nScheduling methods for production systems and transport systems, and route planning"
  },
  {
    "objectID": "teaching/index.html#courses",
    "href": "teaching/index.html#courses",
    "title": "Andrea Ruglioni",
    "section": "",
    "text": "EPFL Fall 2024, TA as PhD student\nThis course gives an introduction to probability theory and stochastic calculus in discrete and continuous time. The fundamental notions and techniques introduced in this course have many applications in finance, for example for option pricing, risk management and optimal portfolio choice.\n\n\n\nKTH Royal Institute of Technology Fall 2023, TA as graduate student\nThe course contents will mainly be based on the following areas:\n\nQueueing models based on Markov processes, including models for queueing networks.\nModels for inventory optimization, deterministic as well as stochastic.\nScheduling methods for production systems and transport systems, and route planning"
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Andrea Ruglioni",
    "section": "",
    "text": "Optimizing wave energy converters: A co-design approach using surrogate modeling\n\n\nA joint optimization of the physical structure and control system of wave energy converters using deep kernel learning as a surrogate model\n\n\n\nWave energy converters\n\n\nOptimal co-design\n\n\nSurrogate modeling\n\n\nDeep kernel learning\n\n\n\n\n\n\nSeptember 2024\n\n\nAndrea Ruglioni, Edoardo Pasta, Nicolàs Faedo, Paolo Brandimarte\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Andrea Ruglioni",
    "section": "",
    "text": "Deep kernel learning\n\n\n\n\n\nDeep Kernel Learning (DKL) combines deep neural networks with Gaussian Processes to model complex data patterns. In this post, we explore DKL and implement a DKL model using the GPyTorch library in Python.\n\n\n\n\n\nJul 10, 2024\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-task Gaussian processes\n\n\n\n\n\nLearn how to model multiple correlated outputs using the intrinsic model of coregionalization (ICM) and the linear model of coregionalization (LMC).\n\n\n\n\n\nJul 8, 2024\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Gaussian processes\n\n\n\n\n\nAn introduction to Gaussian Processes, discussing their mathematical foundations and practical applications in regression tasks.\n\n\n\n\n\nJun 25, 2024\n\n\n17 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/dkl/s2/index.html",
    "href": "blog/dkl/s2/index.html",
    "title": "Multi-task Gaussian processes",
    "section": "",
    "text": "In my previous post on GPs, we discussed the basics of GPs and their applications in regression tasks. Here, we extend the discussion to multi-task GPs, highlighting their benefits and practical implementations. We will provide an intuitive explanation of the concepts and showcase code examples using GPyTorch. Let’s dive in!"
  },
  {
    "objectID": "blog/dkl/s2/index.html#understanding-multi-task-gps",
    "href": "blog/dkl/s2/index.html#understanding-multi-task-gps",
    "title": "Multi-task Gaussian processes",
    "section": "Understanding multi-task GPs",
    "text": "Understanding multi-task GPs\nGaussian processes are a powerful tool for regression and classification tasks, offering a non-parametric way to define distributions over functions. When dealing with multiple related outputs, a multi-task GP can model the dependencies between these tasks, leading to better generalization and predictions.\nMathematically, a Gaussian process is defined as a collection of random variables, any finite number of which have a joint Gaussian distribution. For a set of input points \\(X\\), the corresponding output values \\(f(X)\\) are jointly Gaussian:\n\\[\nf(X) \\sim \\mathcal{N}(m(X), k(X, X))\n\\]\nwhere \\(m(X)\\) is the mean function and \\(k(X, X)\\) is the covariance matrix.\nIn a multitask setting, we aim to model the function \\(f: \\mathcal{X} \\to \\mathbb{R}^T\\), so that we have \\(T\\) outputs, or tasks, \\(\\{f_t(X)\\}_{t=1}^T\\). This means that the mean function is \\(m: \\mathcal{X} \\to \\mathbb{R}^T\\) and the kernel function is \\(k: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}^{T \\times T}\\). How can we model the correlations between these tasks?\n\nIndependent multi-task GP\nA simple independent multioutput GP models each task independently, without considering any correlations between tasks. In this setup, each task has its own GP with its own mean and covariance functions. Mathematically, this can be expressed as:\n\\[\nf_i(X) \\sim \\mathcal{N}(m_i(X), k_i(X, X)) \\qquad i = 1, \\ldots, T,\n\\]\nleading to a block-diagonal covariance matrix \\(k(x, x) = \\text{diag}(k_1(x, x), \\ldots, k_T(x, x))\\). This approach does not leverage any shared information between tasks, which can lead to suboptimal performance, especially when there is limited data for some tasks.\n\n\nIntrinsic model of coregionalization (ICM)\nThe ICM approach generalizes the independent multioutput GP by introducing a coregionalization matrix \\(B\\) that models the correlations between tasks. Specifically, the covariance function in the ICM approach is defined as:\n\\[\nk(x, x') = k_{\\text{input}}(x, x') B,\n\\]\nwhere \\(k_{\\text{input}}\\) is a covariance function defined over the input space (e.g. squared exponential kernel), and \\(B \\in \\mathbb{R}^{T \\times T}\\) is the coregionalization matrix capturing the task-specific covariances. The matrix \\(B\\) is typically parameterized as \\(B = W W^\\mathsf{T}\\), with \\(W \\in \\mathbb{R}^{T \\times r}\\) and \\(r\\) being the rank of the coregionalization matrix. This ensures the kernel is positive semi-definite.\nThe ICM approach can learn the shared structure between tasks. Indeed, the Pearson correlation coefficient between tasks can be expressed as:\n\\[\n\\rho_{ij} = \\frac{B[i, j]}{\\sqrt{B[i, i] B[j, j]}}.\n\\]\n\n\nLinear model of coregionalization (LMC)\nAnother common approach is the LMC model, which extends the ICM by allowing for a wider variety of input kernels. In the LMC model, the covariance function is defined as:\n\\[\nk(x, x') = \\sum_{q=1}^Q k_{\\text{input}}^{(q)}(x, x') B_q,\n\\]\nwhere \\(Q\\) is the number of base kernels, \\(k_{\\text{input}}^{(q)}\\) are the base kernels, and \\(B_q\\) are the coregionalization matrices for each base kernel. This model can capture even more complex correlations between tasks by combining multiple base kernels. We can recover the ICM model by setting \\(Q=1\\)."
  },
  {
    "objectID": "blog/dkl/s2/index.html#noise-modeling",
    "href": "blog/dkl/s2/index.html#noise-modeling",
    "title": "Multi-task Gaussian processes",
    "section": "Noise modeling",
    "text": "Noise modeling\nIn multi-task GPs, we have to consider a multi-output likelihood function that models the noise for each task. The standard likelihood function is typically a multidimensional Gaussian likelihood, which can be expressed as:\n\\[\ny = f(x) + \\epsilon, \\qquad \\epsilon \\sim \\mathcal{N}_T(0, \\Sigma),\n\\]\nwhere \\(y\\) is the observed output, \\(f(x)\\) is the latent function, and \\(\\Sigma\\) is the noise covariance matrix. The flexibility is on the choice of the noise covariance matrix, which can be diagonal \\(\\Sigma = \\text{diag}(\\sigma_1^2, \\ldots, \\sigma_T^2)\\) (independent noise for each task) or full (correlated noise across tasks). The latter is usually represented as \\(\\Sigma = L L^\\mathsf{T}\\), where \\(L \\in \\mathbb{R}^{T \\times r}\\) and \\(r\\) is the rank of the noise covariance matrix. This allows for capturing correlations between the noise terms of different tasks.\nThe final covariance matrix with noise is then given by:\n\\[\nK_n = K + \\mathbf{I} \\otimes \\Sigma,\n\\]\nwhere \\(K\\) is the covariance matrix without noise, \\(\\mathbf{I}\\) is the identity matrix, and \\(\\otimes\\) denotes the Kronecker product. The noise term is added to the diagonal blocks of the covariance matrix."
  },
  {
    "objectID": "blog/dkl/s2/index.html#implementation-with-gpytorch",
    "href": "blog/dkl/s2/index.html#implementation-with-gpytorch",
    "title": "Multi-task Gaussian processes",
    "section": "Implementation with GPyTorch",
    "text": "Implementation with GPyTorch\nLet’s walk through an example of implementing a multitask GP using GPyTorch with the ICM kernel. First of all, we need to install the required packages, including Torch, GPyTorch, matplotlib, and seaborn, numpy.\n\n\nCode\n%pip install torch gpytorch matplotlib seaborn numpy pandas\n\nimport torch\nimport gpytorch\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\n\n\nAfterward, we can define the multitask GP model. We use an ICM kernel (with rank \\(r=1\\)) to capture correlations between tasks. We generate synthetic noisy training data for two tasks (sine and a shifted sine), so to have correlated outputs. The noise covariance matrix is\n\\[\n\\Sigma = \\begin{bmatrix}\n\\sigma_1^2 & \\rho \\sigma_1 \\sigma_2 \\\\\n\\rho \\sigma_1 \\sigma_2 & \\sigma_2^2\n\\end{bmatrix},\n\\]\nwhere \\(\\sigma_1^2 = \\sigma_2^2 = 0.1^2\\) and \\(\\rho = 0.3\\).\nLastly, we train the model and evaluate its performance by plotting the mean predictions and confidence intervals for each task.\n\n\nCode\n# Define the kernel with coregionalization\nclass MultitaskGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood, num_tasks):\n        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.MultitaskMean(\n            gpytorch.means.ConstantMean(), num_tasks=num_tasks\n        )\n        self.covar_module = gpytorch.kernels.MultitaskKernel(\n            gpytorch.kernels.RBFKernel(), num_tasks=num_tasks, rank=1\n        )\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n\n# Training data\nf1 = lambda x:  torch.sin(x * (2 * torch.pi))\nf2 = lambda x: torch.sin((x - 0.1) * (2 * torch.pi))\ntrain_x = torch.linspace(0, 1, 10)\ntrain_y = torch.stack([\n    f1(train_x),\n    f2(train_x)\n]).T\n# Define the noise covariance matrix with correlation = 0.3\nsigma2 = 0.1**2\nSigma = torch.tensor([[sigma2, 0.3 * sigma2], [0.3 * sigma2, sigma2]])\n# Add noise to the training data\ntrain_y += torch.tensor(np.random.multivariate_normal(mean=[0,0], cov=Sigma, size=len(train_x)))\n\n# Model and likelihood\nnum_tasks = 2\nlikelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=num_tasks, rank=1)\nmodel = MultitaskGPModel(train_x, train_y, likelihood, num_tasks)\n\n# Training the model\nmodel.train()\nlikelihood.train()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\nmll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n\nnum_iter = 500\nfor i in range(num_iter):\n    optimizer.zero_grad()\n    output = model(train_x)\n    loss = -mll(output, train_y)\n    loss.backward()\n    optimizer.step()\n    scheduler.step()\n\n# Evaluation\nmodel.eval()\nlikelihood.eval()\n\ntest_x = torch.linspace(0, 1, 100)\n\nwith torch.no_grad(), gpytorch.settings.fast_pred_var():\n    pred_multi = likelihood(model(test_x))\n\n# Plot predictions\nfig, ax = plt.subplots()\n\ncolors = ['blue', 'red']\nfor i in range(num_tasks):\n    ax.plot(test_x, pred_multi.mean[:, i], label=f'Mean prediction (Task {i+1})', color=colors[i])\n    ax.plot(test_x, [f1(test_x), f2(test_x)][i], linestyle='--', label=f'True function (Task {i+1})')\n    lower = pred_multi.confidence_region()[0][:, i].detach().numpy()\n    upper = pred_multi.confidence_region()[1][:, i].detach().numpy()\n    ax.fill_between(\n        test_x,\n        lower,\n        upper,\n        alpha=0.2,\n        label=f'Confidence interval (Task {i+1})',\n        color=colors[i]\n    )\n\nax.scatter(train_x, train_y[:, 0], color='black', label=f'Training data (Task 1)')\nax.scatter(train_x, train_y[:, 1], color='gray', label=f'Training data (Task 2)')\n\nax.set_title('Multitask GP with ICM')\nax.legend(loc='lower center', bbox_to_anchor=(0.5, -0.2),\n          ncol=3, fancybox=True)\n\n\n\n\n\n\n\n\nFigure 1: Predictions of the multitask GP with ICM kernel.\n\n\n\n\n\nUsing GPyTorch, the ICM model is straightforward to implement by using the MultitaskMean, MultitaskKernel, and MultitaskGaussianLikelihood classes. These take care of the multitask structure, the noise and coregionalization matrices, allowing us to focus on the model definition and training.\nRegarding the training loop, it works similarly to standard GPs, with the negative marginal log-likelihood as the loss function, and an optimizer to update the model parameters. A scheduler has been added to reduce the learning rate during training, which can help stabilize the optimization process.\nFigure 2 show the coregionalization matrix \\(B\\) learned by the model, and the noise covariance matrix \\(\\Sigma\\). The former captures the correlations between the tasks. As we can see, the off-diagonal elements of \\(B\\) are positive. The latter represents the noise levels for each task. Notice that the model has properly learned the noise correlation.\n\nCode\nW = model.covar_module.task_covar_module.covar_factor\nB = W @ W.T\n\nfig, ax = plt.subplots()\nsns.heatmap(B.detach().numpy(), annot=True, ax=ax, cbar=False, square=True)\nax.set_xticklabels(['Task 1', 'Task 2'])\nax.set_yticklabels(['Task 1', 'Task 2'])\nax.set_title('Coregionalization matrix B')\nfig.show()\n\n\nL = model.likelihood.task_noise_covar_factor.detach().numpy()\nSigma = L @ L.T\n\nfig, ax = plt.subplots()\nsns.heatmap(Sigma, annot=True, ax=ax, cbar=False, square=True)\nax.set_xticklabels(['Task 1', 'Task 2'])\nax.set_yticklabels(['Task 1', 'Task 2'])\nax.set_title('Noise covariance matrix')\nfig.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Coregionalization matrix \\(B\\) and noise covariance matrix \\(\\Sigma\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\nComparison\nTo highlight the advantages of modeling correlated outputs using the ICM approach, let’s compare it with a model that treats each task independently, ignoring any potential correlations between tasks. We can define a separate GP for each task, train them, and evaluate their performance on the test data.\n\n\nCode\nclass IndependentGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(IndependentGPModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n# Create models and likelihoods for each task\nlikelihoods = [gpytorch.likelihoods.GaussianLikelihood() for _ in range(num_tasks)]\nmodels = [IndependentGPModel(train_x, train_y[:, i], likelihoods[i]) for i in range(num_tasks)]\n\n# Training the independent models\nfor i, (model, likelihood) in enumerate(zip(models, likelihoods)):\n    model.train()\n    likelihood.train()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n    \n    for _ in range(num_iter):\n        optimizer.zero_grad()\n        output = model(train_x)\n        loss = -mll(output, train_y[:, i])\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n# Evaluation\nfor model, likelihood in zip(models, likelihoods):\n    model.eval()\n    likelihood.eval()\n\nwith torch.no_grad(), gpytorch.settings.fast_pred_var():\n    pred_inde = [likelihood(model(test_x)) for model, likelihood in zip(models, likelihoods)]\n\n# Plot predictions\nfig, ax = plt.subplots()\n\nfor i in range(num_tasks):\n    ax.plot(test_x, pred_inde[i].mean, label=f'Mean prediction (Task {i+1})', color=colors[i])\n    ax.plot(test_x, [f1(test_x), f2(test_x)][i], linestyle='--', label=f'True function (Task {i+1})')\n    lower = pred_inde[i].confidence_region()[0]\n    upper = pred_inde[i].confidence_region()[1]\n    ax.fill_between(\n        test_x,\n        lower,\n        upper,\n        alpha=0.2,\n        label=f'Confidence interval (Task {i+1})',\n        color=colors[i]\n    )\n\nax.scatter(train_x, train_y[:, 0], color='black', label='Training data (Task 1)')\nax.scatter(train_x, train_y[:, 1], color='gray', label='Training data (Task 2)')\n\nax.set_title('Independent GPs')\nax.legend(loc='lower center', bbox_to_anchor=(0.5, -0.2),\n          ncol=3, fancybox=True)\n\n\n\n\n\n\n\n\nFigure 3: Predictions of the independent GPs.\n\n\n\n\n\nIn terms of performance, we can compare the mean squared error (MSE) of the predictions on the test data for the multitask GP with ICM and the independent GPs.\n\n\nCode\nmean_multi = pred_multi.mean.numpy()\nmean_inde = np.stack([pred.mean.numpy() for pred in pred_inde]).T\n\ntest_y = torch.stack([f1(test_x), f2(test_x)]).T.numpy()\nMSE_multi = np.mean((mean_multi - test_y) ** 2)\nMSE_inde = np.mean((mean_inde - test_y) ** 2)\n\ndf = pd.DataFrame({\n    'Model': ['ICM', 'Independent'],\n    'MSE': [MSE_multi, MSE_inde]\n  })\ndf\n\n\n\n\n\n\n\n\n\nModel\nMSE\n\n\n\n\n0\nICM\n0.002097\n\n\n1\nIndependent\n0.002626\n\n\n\n\n\n\n\nThe results show that ICM slightly outperforms the independent GPs in terms of MSE, thanks to the shared structure learned by the coregionalization matrix. In practice, the improvement can be more significant when dealing with more complex tasks or limited data. Indeed, in the independent scenario, each GP learns from a smaller dataset of 10 points, potentially leading to overfitting or suboptimal generalization. On the other hand, the multitask GP with ICM uses all the 20 points to learn the squared exponential kernel parameters. This shared information helps to improve the predictions for both tasks."
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Welcome!",
    "section": "",
    "text": "I’m Andrea Ruglioni, a PhD student in Quantitative Finance at the Swiss Finance Institute, EPFL. My research interests lie at the intersection of artifical intelligence, optimization, and finance. I’m passionate about developing novel methodologies to tackle complex problems in quantitative finance and beyond.\nGet in touch by sending me an email"
  },
  {
    "objectID": "about/index.html#education",
    "href": "about/index.html#education",
    "title": "Welcome!",
    "section": "Education",
    "text": "Education\n\n\nPhD in Quantitative Finance\nSwiss Finance Institute, EPFL\n2024 - Present\n\n\nMSc in Mathematical Engineering\nPolitecnico di Torino\n2022 - 2024\n\n\nErasmus+ Exchange Program\nKTH Royal Institute of Technology\n2023 - 2024\n\n\nBSc in Mathematics for Engineering\nPolitecnico di Torino\n2019 - 2022"
  },
  {
    "objectID": "blog/dkl/s1/index.html",
    "href": "blog/dkl/s1/index.html",
    "title": "Introduction to Gaussian processes",
    "section": "",
    "text": "Welcome to the first installment of our series on deep kernel learning. In this post, we’ll delve into Gaussian processes (GPs) and their application as regressors. We’ll start by exploring what GPs are and why they are powerful tools for regression tasks. In subsequent posts, we’ll build on this foundation to discuss multi-task Gaussian processes and how they can be combined with neural networks to create deep kernel models."
  },
  {
    "objectID": "blog/dkl/s1/index.html#gaussian-processes",
    "href": "blog/dkl/s1/index.html#gaussian-processes",
    "title": "Introduction to Gaussian processes",
    "section": "Gaussian processes",
    "text": "Gaussian processes\nTo understand Gaussian processes fully, it’s important to briefly mention the Kolmogorov extension theorem. This theorem guarantees the existence of a stochastic process, i.e., a collection of random variables \\(\\{Y_x\\}_{x \\in \\mathcal{X}}, Y_x \\in \\mathbb{R}\\), that satisfies a specified finite-dimensional distribution. For instance, it ensures that we can define a Gaussian process by specyfing that any finite set of random variables has a multivariate Gaussian distribution, without worrying about the infinite-dimensional nature of the process. Observe that, in a similar matter, we could define a t-student process, by imposing that finite-dimensional distributions are t-student.\nTherefore, similar to a multivariate Gaussian distribution, a Gaussian process \\(f\\) is defined by its mean function \\(m(\\cdot) : \\mathcal{X} \\to \\mathbb{R}\\) and covariance function \\(k(\\cdot, \\cdot) : \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}\\):\n\\[\nf \\sim GP(m, k),\n\\]\nand it can be interpreted as an infinite-dimensional generalization of a multivariate Gaussian distribution.\nIn a regression setting, we could use \\(f\\) as a surrogate model of a function \\(g: \\mathcal{X} \\to \\mathbb{R}\\), where \\(\\mathcal{X}\\) is the input space. Suppose you have a set of input points \\(X = \\{x_1, x_2, \\ldots, x_n\\}\\), with observations \\(Y = \\{y_1 = g(x_1), y_2 = g(x_2), \\ldots, y_n = g(x_n)\\}\\), the joint distribution of the observed outputs \\(Y\\), assuming a GP prior, is given by:\n\\[\n\\begin{pmatrix}\ny_1 \\\\\n\\vdots \\\\\ny_n\n\\end{pmatrix} \\sim \\mathcal{N}\\left(\n\\mathbf{m} =\n\\begin{pmatrix}\nm(x_1) \\\\\n\\vdots \\\\\nm(x_n)\n\\end{pmatrix},\n\\mathbf{K} =\n\\begin{pmatrix}\nk(x_1, x_1) & \\dots & k(x_1, x_n) \\\\\n\\vdots & \\ddots & \\vdots \\\\\nk(x_n, x_1) & \\dots & k(x_n, x_n)\n\\end{pmatrix}\\right),\n\\]\nwhere \\(\\mathbf{m}\\) is the vector of mean function values and \\(\\mathbf{K}\\) is the covariance matrix of the function values at the input points. This approach allows us to make predictions at new input points \\(x_*\\) by conditioning on the observed data, providing not only point estimates but also uncertainty estimates.\n\nMaking predictions\nTo make a prediction \\(y_* = g(x_*)\\) at new input point, we use the joint distribution of the observed outputs \\(Y\\) and the function values at \\(x_*\\), which is given by:\n\\[\n\\begin{pmatrix}\nY \\\\\ny_*\n\\end{pmatrix} \\sim \\mathcal{N}\\left(\n\\begin{pmatrix}\n\\mathbf{m} \\\\\nm(x_*)\n\\end{pmatrix},\n\\begin{pmatrix}\n\\mathbf{K} & k(X, x_*) \\\\\nk(x_*, X) & k(x_*, x_*)\n\\end{pmatrix}\\right)\n\\]\nwhere \\(k(x_*, X)\\) is vector of covariances between the new input point \\(x_*\\) and the observed data points \\(X\\). The conditional distribution of \\(y_*\\) given \\(Y\\) is then Gaussian with mean and covariance:\n\\[\n\\mu(x_* \\mid X, Y) = k(x_*, X) \\mathbf{K}^{-1} (Y - \\mathbf{m}),\n\\] \\[\ns^2(x_* \\mid X, Y) = k(x_*, x_*) - k(x_*, X) \\mathbf{K}^{-1} k(X, x_*).\n\\]\nTherefore, given the observed data, we can estimate the function value at a new input point \\(x_*\\) as \\(\\mu(x_*)\\) and quantify the uncertainty in the prediction as \\(s^2(x_*)\\). This is a key advantage of GPs, which can be important in decision-making processes."
  },
  {
    "objectID": "blog/dkl/s1/index.html#interactive-visualizations",
    "href": "blog/dkl/s1/index.html#interactive-visualizations",
    "title": "Introduction to Gaussian processes",
    "section": "Interactive visualizations",
    "text": "Interactive visualizations\nLet’s explore some interactive plots to better understand how the kernel functions influence the Gaussian process model. Indeed, the choice of the kernel function is crucial in defining the prior over functions, as it determines the smoothness and periodicity of the functions that the GP can model. Therefore, they play a fundamental role in the model’s flexibility and generalization capabilities, and they can be tailored to the specific characteristics of the data at hand. On the other hand, the mean function is usually set constant, as the kernel is flexible enough.\n\nSquared exponential kernel\nThe squared exponential kernel (also known as the RBF kernel) is defined as:\n\\[\nk_{\\text{Exp}}(x, x') = \\sigma^2 \\exp \\left( -\\frac{(x - x')^2}{2l^2} \\right)\n\\]\nwhere \\(\\sigma^2\\) is the variance and \\(l\\) is the length scale. Below is an interactive plot that shows how the squared exponential kernel depends on the lengthscale and variance. Notice that with a small length scale, the function is more wiggly. Instead, with a large length scale it is smoother, as the kernel function decays more slowly with distance (i.e., the correlation between faraway points is higher, and they are more similar to each other). Instead, the variance controls the amplitude of the function, with higher values leading to more variability.\n#| standalone: true\n#| viewerHeight: auto\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\nfrom shiny import App, render, ui\n\n\n# Define the kernel function\ndef exponential_quadratic_kernel(x1, x2, l=1.0, sigma2=1.0):\n    \"\"\"Computes the exponential quadratic kernel (squared exponential kernel).\"\"\"\n    sqdist = np.sum(x1**2, 1).reshape(-1, 1) + np.sum(x2**2, 1) - 2 * np.dot(x1, x2.T)\n    return sigma2 * np.exp(-0.5 / l**2 * sqdist)\n\n# Define the input space\nX = np.linspace(-4, 4, 100).reshape(-1, 1)\n\ndef plot_kernel_and_samples(l, sigma2):\n    # Compute the kernel matrix\n    K = exponential_quadratic_kernel(X, X, l=l, sigma2=sigma2)\n\n    # Create the plot\n    fig, ax = plt.subplots(2, 1, figsize=(18, 6), sharex=True)\n\n    # Plot the kernel\n    ax[0].plot(X, exponential_quadratic_kernel(X, np.zeros((1,1)), l=l, sigma2=sigma2))\n    ax[0].set_title(f\"Squared exponential kernel function\")\n\n    # Sample 5 functions from the Gaussian process defined by the kernel\n    mean = np.zeros(100)\n    cov = K\n    samples = multivariate_normal.rvs(mean, cov, 5)\n\n    # Plot the samples\n    for i in range(5):\n        ax[1].plot(X, samples[i])\n    ax[1].set_title(\"Samples from the GP\")\n    ax[1].set_xlabel(\"x\")\n\n    plt.tight_layout()\n    return fig\n\napp_ui = ui.page_fluid(\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_slider(\"length_scale\", \"Length Scale (l):\", min=0.1, max=5.0, value=1.0, step=0.1),\n            ui.input_slider(\"variance\", \"Variance (σ²):\", min=0.1, max=5.0, value=1.0, step=0.1)\n        ),\n        ui.panel_main(\n            ui.output_plot(\"kernelPlot\")\n        )\n    )\n)\n\ndef server(input, output, session):\n    @output\n    @render.plot\n    def kernelPlot():\n        l = input.length_scale()\n        sigma2 = input.variance()\n        fig = plot_kernel_and_samples(l, sigma2)\n        return fig\n\napp = App(app_ui, server)\n\n\nMatérn kernel\nThe Matérn kernel is defined as:\n\\[\nk_{\\text{Matérn}}(x, x') = \\sigma^2 \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left( \\sqrt{2\\nu} \\frac{|x - x'|}{l} \\right)^{\\nu} K_{\\nu} \\left( \\sqrt{2\\nu} \\frac{|x - x'|}{l} \\right)\n\\]\nwhere \\(\\nu\\) controls the smoothness of the function, \\(l\\) is the length scale, \\(\\sigma^2\\) is the variance, and \\(K_{\\nu}\\) is the modified Bessel function. The former two parameters have the same effect as in the squared exponential kernel, while \\(\\nu\\) controls the smoothness of the function. Indeed, we have that the samples generated have smoothness \\(\\lceil \\nu \\rceil - 1\\), and for \\(\\nu \\to \\infty\\), the Matérn kernel converges to the squared exponential kernel, leading to infinitely smooth functions.\n#| standalone: true\n#| viewerHeight: 475\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\nfrom shiny import App, render, ui\nfrom scipy.special import kv, gamma\nfrom scipy.spatial.distance import cdist\n\n\n# Define the kernel function\ndef matern_kernel(x1, x2, l=1.0, sigma2=1.0, nu=1.5):\n    \"\"\"Computes the Matérn kernel.\"\"\"\n    D = cdist(x1, x2, 'euclidean')\n    const = (2**(1-nu))/gamma(nu)\n    K = const * (np.sqrt(2*nu)*D/l)**nu * kv(nu, np.sqrt(2*nu)*D/l)\n    # Replace NaN values with 1 for x == x'\n    K[np.isnan(K)] = 1\n    K *= sigma2\n    return K\n\n# Define the input space\nX = np.linspace(-4, 4, 100).reshape(-1, 1)\n\ndef plot_kernel_and_samples(l, sigma2, nu):\n    # Compute the kernel matrix\n    K = matern_kernel(X, X, l=l, sigma2=sigma2, nu=nu)\n\n    # Create the plot\n    fig, ax = plt.subplots(2, 1, figsize=(18, 6), sharex=True)\n\n    # Plot the kernel\n    ax[0].plot(X, matern_kernel(X, np.zeros((1, 1)), l=l, sigma2=sigma2, nu=nu))\n    ax[0].set_title(f\"Matern kernel function\")\n\n    # Sample 5 functions from the Gaussian process defined by the kernel\n    mean = np.zeros(100)\n    cov = K\n    samples = multivariate_normal.rvs(mean, cov, 5)\n\n    # Plot the samples\n    for i in range(5):\n        ax[1].plot(X, samples[i])\n    ax[1].set_title(\"Samples from the GP\")\n    ax[1].set_xlabel(\"x\")\n\n    plt.tight_layout()\n    return fig\n\napp_ui = ui.page_fluid(\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_slider(\"length_scale\", \"Length Scale (l):\", min=0.1, max=5.0, value=1.0, step=0.1),\n            ui.input_slider(\"variance\", \"Variance (σ²):\", min=0.1, max=5.0, value=1.0, step=0.1),\n            ui.input_slider(\"smoothness_param\", \"Smoothness param (v):\", min=1.5, max=5.0, value=1.5, step=0.5)\n        ),\n        ui.panel_main(\n            ui.output_plot(\"kernelPlot\")\n        )\n    )\n)\n\ndef server(input, output, session):\n    @output\n    @render.plot\n    def kernelPlot():\n        l = input.length_scale()\n        sigma2 = input.variance()\n        nu = input.smoothness_param()\n        fig = plot_kernel_and_samples(l, sigma2, nu)\n        return fig\n\napp = App(app_ui, server)"
  },
  {
    "objectID": "blog/dkl/s1/index.html#noisy-observations",
    "href": "blog/dkl/s1/index.html#noisy-observations",
    "title": "Introduction to Gaussian processes",
    "section": "Noisy observations",
    "text": "Noisy observations\nNoise and measurement error are inevitable in real-world data, which can significantly impact the performance and reliability of predictive models. As a result, it is essential to take noise into account when modeling data with GP. We can represent noisy observations as:\n\\[\ny = g(x) + \\epsilon\n\\]\nwhere \\(\\epsilon\\) is a random variable representing the noise. Usually, we assume that \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma_n^2)\\), where \\(\\sigma_n^2\\) is the variance of the noise. In this way, it can be easily incorporated into the GP by adding a diagonal noise term to the kernel matrix:\n\\[\n\\mathbf{K}_n = \\mathbf{K} + \\sigma_n^2 \\mathbf{I},\n\\]\nwhere \\(\\mathbf{K}\\) is the kernel matrix computed on the training data and \\(\\mathbf{I}\\) is the identity matrix.\nBelow is an interactive plot that demonstrates how noise influences the GP model. The plot shows the noisy training data (black points) of the true function \\(g(x) = \\sin(x)\\), the red dashed line. The plot also shows the GP mean prediction (blue line) for the squared exponential and Matérn kernels, along with the 95% confidence intervals. For \\(\\sigma_n^2 = 0\\), the model perfectly interpolates the training data. For higher noise levels, the model becomes less certain about the observations, leading to a non-interpolating behavior, and the confidence intervals widen.\n#| standalone: true\n#| viewerHeight: 475\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\nfrom shiny import App, render, ui\nfrom scipy.spatial.distance import cdist\nfrom scipy.special import kv, gamma\n\n# Define the kernel functions\ndef exponential_quadratic_kernel(x1, x2, l=1.0, sigma2=1.0):\n    \"\"\"Computes the exponential quadratic kernel (squared exponential kernel).\"\"\"\n    sqdist = np.sum(x1**2, 1).reshape(-1, 1) + np.sum(x2**2, 1) - 2 * np.dot(x1, x2.T)\n    return sigma2 * np.exp(-0.5 / l**2 * sqdist)\n\ndef matern_kernel(x1, x2, l=1.0, sigma2=1.0, nu=1.5):\n    \"\"\"Computes the Matérn kernel.\"\"\"\n    D = cdist(x1, x2, 'euclidean')\n    const = (2**(1-nu))/gamma(nu)\n    K = sigma2 * const * (np.sqrt(2*nu)*D/l)**nu * kv(nu, np.sqrt(2*nu)*D/l)\n    # Replace NaN values with 1 for x == x'\n    K[np.isnan(K)] = 1\n    return K\n\n# Define the input space\nX = np.linspace(-4, 4, 100).reshape(-1, 1)\n\ndef plot_kernels_and_samples(noise_level):\n    # Generate training data with noise\n    X_train = np.array([-3, -2, -1, 1, 3.5]).reshape(-1, 1)\n    y_train = np.sin(X_train) + noise_level * np.random.randn(X_train.shape[0], 1).reshape(-1,1)\n\n    # Compute the kernel matrices for training data\n    K_train_exp = exponential_quadratic_kernel(X_train, X_train) + noise_level**2 * np.eye(len(X_train))\n    K_s_exp = exponential_quadratic_kernel(X_train, X)\n    K_ss_exp = exponential_quadratic_kernel(X, X)\n    \n    K_train_matern = matern_kernel(X_train, X_train) + noise_level**2 * np.eye(len(X_train))\n    K_s_matern = matern_kernel(X_train, X)\n    K_ss_matern = matern_kernel(X, X)\n\n    # Compute the mean and covariance of the posterior distribution for exponential kernel\n    K_train_inv_exp = np.linalg.inv(K_train_exp)\n    mu_s_exp = (K_s_exp.T.dot(K_train_inv_exp).dot(y_train)).reshape(-1)\n    cov_s_exp = K_ss_exp - K_s_exp.T.dot(K_train_inv_exp).dot(K_s_exp)\n\n    # Compute the mean and covariance of the posterior distribution for Matérn kernel\n    K_train_inv_matern = np.linalg.inv(K_train_matern)\n    mu_s_matern = (K_s_matern.T.dot(K_train_inv_matern).dot(y_train)).reshape(-1)\n    cov_s_matern = K_ss_matern - K_s_matern.T.dot(K_train_inv_matern).dot(K_s_matern)\n\n    # Sample 5 functions from the posterior distribution for both kernels\n    samples_exp = multivariate_normal.rvs(mu_s_exp, cov_s_exp, 5)\n    samples_matern = multivariate_normal.rvs(mu_s_matern, cov_s_matern, 5)\n\n    # Create the plot\n    fig, ax = plt.subplots(2, 1, figsize=(18, 6), sharex=True)\n\n    # Plot the training data and GP predictions for exponential kernel\n    ax[0].scatter(X_train, y_train, color='black', zorder=10, label='Noisy observations')\n    ax[0].plot(X, mu_s_exp, color='blue', label='Mean prediction')\n    ax[0].plot(X, np.sin(X), color='r', linestyle='--', label='True function')\n    ax[0].fill_between(X.ravel(), mu_s_exp - 1.96 * np.sqrt(np.diag(cov_s_exp)), mu_s_exp + 1.96 * np.sqrt(np.diag(cov_s_exp)), color=\"blue\", alpha=0.2, label='Confidence interval')\n    # for i in range(5):\n    #     ax[0].plot(X, samples_exp[i], alpha=0.5, linestyle='--')\n    ax[0].set_title(f\"Squared exponential kernel\")\n    # ax[0].legend()\n\n    # Plot the training data and GP predictions for Matérn kernel\n    ax[1].scatter(X_train, y_train, color='black', zorder=10, label='Noisy observations')\n    ax[1].plot(X, mu_s_matern, color=\"blue\", label='Mean prediction')\n    ax[1].plot(X, np.sin(X), color=\"r\", linestyle='--', label='True function')\n    ax[1].fill_between(X.ravel(), mu_s_matern - 1.96 * np.sqrt(np.diag(cov_s_matern)), mu_s_matern + 1.96 * np.sqrt(np.diag(cov_s_matern)), color=\"blue\", alpha=0.2, label='Confidence interval')\n    # for i in range(5):\n    #     ax[1].plot(X, samples_matern[i], alpha=0.5, linestyle='--')\n    ax[1].set_title(f\"Matérn kernel\")\n    # ax[1].legend()\n    ax[1].set_xlabel(\"x\")\n\n    plt.tight_layout()\n    return fig\n\napp_ui = ui.page_fluid(\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_slider(\"noise_level\", \"Noise Level (σ²ₙ):\", min=0.0, max=1.0, value=0.0, step=0.01)\n        ),\n        ui.panel_main(\n            ui.output_plot(\"kernelPlot\")\n        )\n    )\n)\n\ndef server(input, output, session):\n    @output\n    @render.plot\n    def kernelPlot():\n        noise_level = input.noise_level()\n        fig = plot_kernels_and_samples(noise_level)\n        return fig\n\napp = App(app_ui, server)"
  },
  {
    "objectID": "blog/dkl/s1/index.html#code",
    "href": "blog/dkl/s1/index.html#code",
    "title": "Introduction to Gaussian processes",
    "section": "Code",
    "text": "Code\nIn this section, we’ll provide a brief overview of how to implement Gaussian processes in Python using the GPyTorch library. The code snippet below demonstrates how to define a GP model with a squared exponential kernel and train it on synthetic data.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport gpytorch\nfrom gpytorch.kernels import RBFKernel, ScaleKernel\nfrom gpytorch.means import ConstantMean\nfrom gpytorch.likelihoods import GaussianLikelihood\nfrom gpytorch.models import ExactGP\nfrom gpytorch.mlls import ExactMarginalLogLikelihood\nfrom gpytorch.distributions import MultivariateNormal\n\n# Define the GP model\nclass GP(ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(GP, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = ConstantMean()\n        self.covar_module = ScaleKernel(RBFKernel())\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return MultivariateNormal(mean_x, covar_x)\n\n# Generate synthetic data\nf = lambda x: torch.sin(x * (2 * np.pi))\ntrain_x = torch.linspace(0, 1, 10)\ntrain_y = f(train_x) + torch.randn(train_x.size()) * 0.1\nlikelihood = GaussianLikelihood()\n\n# Initialize the model and likelihood\nmodel = GP(train_x, train_y, likelihood)\n\n# Training the model\nmodel.train()\nlikelihood.train()\n\n# Use the adam optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\n# \"Loss\" for GPs - the marginal log likelihood\nmll = ExactMarginalLogLikelihood(likelihood, model)\n\n# Training loop\ntraining_iterations = 50\nfor i in range(training_iterations):\n    optimizer.zero_grad()\n    output = model(train_x)\n    loss = -mll(output, train_y)\n    loss.backward()\n    optimizer.step()\n\n# Set the model and likelihood into evaluation mode\nmodel.eval()\nlikelihood.eval()\n\n# Make predictions\ntest_x = torch.linspace(0, 1, 100)\nwith torch.no_grad(), gpytorch.settings.fast_pred_var():\n    observed_pred = likelihood(model(test_x))\n    mean = observed_pred.mean\n    lower, upper = observed_pred.confidence_region()\n\n# Plot the results\nfig, ax = plt.subplots()\nax.scatter(train_x, train_y, color='black', label='Training data')\nax.plot(test_x, f(test_x), 'r--', label='True function')\nax.plot(test_x, mean, 'b', label='Mean prediction')\nax.fill_between(test_x, lower, upper, alpha=0.2, color='blue', label='Confidence interval')\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.legend()\n\n\n\n\n\n\n\n\n\nThe three main components that need to be defined are the mean function, the kernel function, and the likelihood function (which models the noise in the data). Observe that the base kernel is the RBFKernel, which corresponds to the squared exponential kernel, and it is wrapped by the ScaleKernel to allow for the scaling of the kernel through the variance parameter \\(\\sigma^2\\). The ExactMarginalLogLikelihood object is used to compute the marginal log likelihood, which is the negative loss function for training the GP model. Indeed, the GP model parameters are optimized by maximizing the marginal log likelihood of the observed data, which is given by\n\\[\n\\mathcal{L}(\\theta) = \\log p(Y \\mid X, \\theta)\n    = -\\frac{1}{2} (Y - \\mathbf{m})^\\mathsf{T} \\mathbf{K}_n^{-1} (Y - \\mathbf{m}) - \\frac{1}{2} \\log |\\mathbf{K}_n| - \\frac{N}{2} \\log(2\\pi),\n\\]\nwhere \\(\\theta\\) are the model parameters, comprising the mean, kernel, and likelihood parameters. Computaionally speaking, the inversion of the kernel matrix \\(\\mathbf{K}_n\\) is the most expensive operation, with a complexity of \\(\\mathcal{O}(n^3)\\), where \\(n\\) is the number of training points. Therefore, for very large datasets, approximate inference methods, inducing points, or sparse GPs should be used to reduce the computational burden.\nLastly, observe that the ExactGP class is the standard GP for Gaussian likelihoods, where the exact marginal log likelihood can be computed in closed form. However, GPyTorch also provides different likelihoods, such as student-t likelihoods (which is more stable if outliers are present) and more. In these cases, the must class ApproximateGP should be used, which allows for approximate inference methods like variational inference. Regarding the loss function, the ExactMarginalLogLikelihood should be replaced by the VariationalELBO object, or other appropriate loss functions for approximate inference."
  },
  {
    "objectID": "blog/dkl/s1/index.html#applications",
    "href": "blog/dkl/s1/index.html#applications",
    "title": "Introduction to Gaussian processes",
    "section": "Applications",
    "text": "Applications\n\nBayesian hyperparameter tuning\nHyperparameter tuning is a critical yet challenging aspect of training neural networks. Finding the optimal combination of hyperparameters, such as learning rate, batch size, number of layers, and units per layer, can significantly enhance a model’s performance. Traditional methods like grid search and random search often prove to be inefficient and computationally expensive. This is where Bayesian optimization, powered by GPs, comes into play, offering a smarter approach to hyperparameter tuning.\nUnlike exhaustive search methods, Bayesian optimization is more sample-efficient, meaning it can find optimal hyperparameters with fewer iterations. It works by\n\nmodeling the objective function (e.g., validation loss) as a GP in the hyperparameter space.\nusing an acquisition function to decide where to sample next. The acquisition function balances exploration (sampling in unexplored regions) and exploitation (sampling in regions with low loss) to guide the search towards the global optimum.\n\n\n\nSurrogate optimization\nGPs are also used in surrogate optimization, where the objective function is expensive to evaluate, and we aim to find the global optimum with as few evaluations as possible. By modeling the objective function as a GP, we can make informed decisions about where to sample next, focusing on regions that are likely to contain the global optimum. This can significantly reduce the number of evaluations needed to find the best solution.\n\n\nTime series forecasting\nGPs are also widely used in time series forecasting due to their flexibility and ability to model complex patterns in the data. By treating time series data as a function of time, Gaussian processes can capture the underlying dynamics and dependencies in the series. They can provide not only point estimates but also probabilistic forecasts, including prediction intervals that quantify uncertainty."
  },
  {
    "objectID": "blog/dkl/s3/index.html",
    "href": "blog/dkl/s3/index.html",
    "title": "Deep kernel learning",
    "section": "",
    "text": "In this post, we explore deep kernel learning (DKL), a hybrid approach that combines the strengths of deep neural networks (DNNs) with Gaussian processes (GPs). DKL offers a powerful framework for modeling complex data patterns, enhancing the predictive capabilities and interpretability of standard GPs. If you are new to Gaussian processes, we recommend reading our previous posts on Gaussian processes and Multi-output Gaussian processes before diving into DKL."
  },
  {
    "objectID": "blog/dkl/s3/index.html#gaussian-processes-gps",
    "href": "blog/dkl/s3/index.html#gaussian-processes-gps",
    "title": "Deep kernel learning",
    "section": "Gaussian processes (GPs)",
    "text": "Gaussian processes (GPs)\nGaussian processes are non-parametric models used for regression and classification tasks. A GP is defined by its mean function \\(m(\\cdot)\\) and covariance function (kernel) \\(k(\\cdot, \\cdot)\\).\n\\[\nf \\sim \\mathcal{GP}(m, k)\n\\]\nRemember that, given training data \\(X = [x_1, \\ldots, x_n]\\) and targets \\(Y = [y_1 = g(x_1), \\ldots, y_n = g(x_n)]\\), the predictive distribution for a test point \\(x_*\\) is:\n\\[\np(f_* \\mid X, Y, \\mathbf{x}_*) = \\mathcal{N}(\\mu(x_* \\mid X, Y), s^2(x_* \\mid X, Y))\n\\]\nwhere:\n\\[\n\\mu(x_* \\mid X, Y) = m(x_*) + k(x_*, X) K_n^{-1} (Y - m(X))\n\\]\n\\[\ns(x_* \\mid X, Y) = k(x_*, x_*) - k(x_*, X) K_n^{-1} k(X, x_*)\n\\]\nand \\(K_n\\) is the kernel matrix for training data \\(X\\) with noise added to the diagonal.\nHere, we can notice how much the kernel is important. It plays a vital role both in the mean and the variance of the predictive distribution (that is why often the mean is set to zero). Therefore, its choice is crucial for the model’s performance, and ad-hoc kernels are designed to capture specific patterns in the data."
  },
  {
    "objectID": "blog/dkl/s3/index.html#deep-kernel-learning-dkl",
    "href": "blog/dkl/s3/index.html#deep-kernel-learning-dkl",
    "title": "Deep kernel learning",
    "section": "Deep kernel learning (DKL)",
    "text": "Deep kernel learning (DKL)\nDKL integrates DNNs with GPs by using a DNN to learn a representation of the data, which is then used to define the GP kernel. This allows the GP to capture complex patterns in the data that a standard kernel might miss.\nIn DKL, the DNN acts as a feature extractor, transforming input \\(X\\) into a feature vector \\(Z = \\phi(X)\\). The GP kernel is then defined on these features:\n\\[\nk_{\\phi}(\\mathbf{x}, \\mathbf{x}') = k(\\phi(\\mathbf{x}), \\phi(\\mathbf{x}'))\n\\]\nHere stands the flexibility of DKL. The most suitable kernel is learnt ad-hoc for the data at hand, and it is not fixed a priori. Indeed, the DNN parameters and the GP ones are jointly optimized to maximize the likelihood of the data.\n\nImplementation with GPyTorch\nLet’s try to implement a DKL model using the GPyTorch library in Python. Then we will compare its performance with a standard GP. We will use the McCormick function as a synthetic dataset and compare the DKL model’s performance with a standard GP. The McCormick function is defined as:\n\\[\ng(x_1, x_2) = \\sin(x_1 + x_2) + (x_1 - x_2)^2 - 1.5x_1 + 2.5x_2 + 1\n\\]\nand it has been widely used as a benchmark function for optimization and regression tasks. To make the proplem more challenging, gaussian noise is added to the training data, as a standard normal distribution with mean 0 and standard deviation 1. Moreover, 25 training points are used to fit the model, distributed on a 5x5 grid. Note that such grid is not fine-grained, as the domain of the function is \\(x_1 \\in [-1.5, 4]\\) and \\(x_2 \\in [-3, 4]\\). Therefore, the model has to generalize well to make accurate predictions on unseen data.\nFirst of all, we need to import the libraries that we need.\n\n\nCode\n%pip install plotly\n\nimport numpy as np\nimport torch\nimport gpytorch\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom torch import nn\nfrom plotly.subplots import make_subplots\nfrom sklearn.metrics import mean_squared_error\n\n\nNow, we can define the McCormick function and generate the synthetic dataset for training and testing.\n\n\n\n\n\n\n\n\nFigure 1: Neural network architecture\n\n\n\nRegarding the deep neural network, we define a simple feedforward network with two hidden layers and a softplus activation function. The architecture is defined in the FeatureExtractor class, and it is shown in Figure 1. The remaining part is the same as the standard GP model, and it is deeply explained in the Gaussian processes post.\n\n\nCode\n# Define the McCormick function\ndef mccormick(x1, x2):\n    return np.sin(x1 + x2) + (x1 - x2)**2 - 1.5 * x1 + 2.5 * x2 + 1\n\n# Generate grid data for plotting\nx1 = np.linspace(-1.5, 4, 100)\nx2 = np.linspace(-3, 4, 100)\nx1_test, x2_test = np.meshgrid(x1, x2)\ny = mccormick(x1_test, x2_test)\n\n# Generate test data\ntest_x = torch.tensor(np.vstack((x1_test.ravel(), x2_test.ravel())).T, dtype=torch.float32)\ntest_y = torch.tensor(y.ravel(), dtype=torch.float32)\n\n# Generate training data\nx1 = np.linspace(-1.5, 4, 5)\nx2 = np.linspace(-3, 4, 5)\nx1_train, x2_train = np.meshgrid(x1, x2)\ntrain_x = np.vstack((x1_train.ravel(), x2_train.ravel())).T\n# add noise to the training data\ntrain_y = mccormick(train_x[:, 0], train_x[:, 1]) + np.random.normal(0, 1, len(train_x))\n\n# Convert training data to tensors\ntrain_x = torch.tensor(train_x, dtype=torch.float32)\ntrain_y = torch.tensor(train_y.ravel(), dtype=torch.float32)\n\n# Define the DNN feature extractor\nclass FeatureExtractor(nn.Module):\n    def __init__(self):\n        super(FeatureExtractor, self).__init__()\n        self.fc1 = nn.Linear(2, 8)\n        self.fc2 = nn.Linear(8, 4)\n        self.fc3 = nn.Linear(4, 2)\n        self.activation = nn.Softplus()\n    \n    def forward(self, x):\n        x = self.activation(self.fc1(x))\n        x = self.activation(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n# Define the DKL model\nclass DKLModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(DKLModel, self).__init__(train_x, train_y, likelihood)\n        feature_extractor = FeatureExtractor()\n        self.feature_extractor = feature_extractor\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n    \n    def forward(self, x):\n        projected_x = self.feature_extractor(x)\n        mean_x = self.mean_module(projected_x)\n        covar_x = self.covar_module(projected_x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n# Define the GP model\nclass GPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(GPModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n# Initialize likelihood and models\ndkl_likelihood = gpytorch.likelihoods.GaussianLikelihood()\ndkl_model = DKLModel(train_x, train_y, dkl_likelihood)\ngp_likelihood = gpytorch.likelihoods.GaussianLikelihood()\ngp_model = GPModel(train_x, train_y, gp_likelihood)\n\n# Training parameters\ntraining_iterations = 500\n\n# Train DKL model\ndkl_model.train()\ndkl_likelihood.train()\noptimizer = torch.optim.Adam([\n    {'params': dkl_model.feature_extractor.parameters()},\n    {'params': dkl_model.covar_module.parameters()},\n    {'params': dkl_model.mean_module.parameters()},\n    {'params': dkl_likelihood.parameters(), 'lr': 0.05},\n], lr=0.1)\nmll = gpytorch.mlls.ExactMarginalLogLikelihood(dkl_likelihood, dkl_model)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\nfor i in range(training_iterations):\n    optimizer.zero_grad()\n    output = dkl_model(train_x)\n    loss = -mll(output, train_y)\n    loss.backward()\n    optimizer.step()\n    scheduler.step()\n\n# Train GP model\ngp_model.train()\ngp_likelihood.train()\noptimizer = torch.optim.Adam([\n    {'params': gp_model.covar_module.parameters()},\n    {'params': gp_model.mean_module.parameters()},\n    {'params': gp_likelihood.parameters()},\n], lr=0.1)\nmll = gpytorch.mlls.ExactMarginalLogLikelihood(gp_likelihood, gp_model)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\nfor i in range(training_iterations):\n    optimizer.zero_grad()\n    output = gp_model(train_x)\n    loss = -mll(output, train_y)\n    loss.backward()\n    optimizer.step()\n    scheduler.step()\n\n# Evaluate models\ndkl_model.eval()\ndkl_likelihood.eval()\ngp_model.eval()\ngp_likelihood.eval()\n\nwith torch.no_grad(), gpytorch.settings.fast_pred_var():\n    dkl_pred = dkl_likelihood(dkl_model(test_x))\n    gp_pred = gp_likelihood(gp_model(test_x))\n\ndkl_pred = dkl_pred.mean.numpy().reshape(x1_test.shape)\ngp_pred = gp_pred.mean.numpy().reshape(x1_test.shape)\ntrue_y = test_y.numpy().reshape(x1_test.shape)\n\n# Create interactive 3D plots\nfig = go.Figure()\n\n# DKL predictions surface\nfig.add_trace(go.Surface(z=dkl_pred, x=x1_test, y=x2_test, colorscale='Blues', opacity=0.6, name='DKL', showscale=False))\n\n# GP predictions surface\nfig.add_trace(go.Surface(z=gp_pred, x=x1_test, y=x2_test, colorscale='Greens', opacity=0.6, name='GP', showscale=False))\n\n# True surface\nfig.add_trace(go.Surface(z=true_y, x=x1_test, y=x2_test, colorscale='Reds', opacity=0.6, name='McCormick', showscale=False))\n\n# Add training points\nfig.add_trace(go.Scatter3d(x=train_x[:, 0].numpy(), y=train_x[:, 1].numpy(), z=train_y.numpy(), mode='markers', marker=dict(size=3, color='black', opacity=0.8), name='Train Points'))\n\n# fig.update_layout(scene=dict(xaxis_title='X1', yaxis_title='X2', zaxis_title='Y'))\nfig.show()\n\n\n                                                \n\n\nThe interactive 3D plot above shows the predictions of the DKL (blue) and GP (green) models on the McCormick function (red). The DKL model captures the complex patterns of the function more accurately than the GP model, providing a better fit to the true surface.\nIn terms of performance, we can compare the mean squared error (MSE) of the DKL and GP models on the test data (i.e., the McCormick function evaluated on a grid 100x100).\n\n\nCode\ndkl_mse = mean_squared_error(test_y.numpy(), dkl_pred.ravel())\ngp_mse = mean_squared_error(test_y.numpy(), gp_pred.ravel())\n\n\ndf = pd.DataFrame({'Model': ['DKL', 'GP'],\n                   'MSE': [dkl_mse, gp_mse]\n                   })\ndf\n\n\n\n\n\n\n\n\n\nModel\nMSE\n\n\n\n\n0\nDKL\n0.802505\n\n\n1\nGP\n4.195485\n\n\n\n\n\n\n\nThe results are clear - the DKL model outperforms the GP model, demonstrating the benefits of combining deep neural networks with Gaussian Processes for complex regression tasks.\n\n\nNN’s embedding\nLet’s visualize the embedding of the input data into the feature space learned by the DNN.\n\n\nCode\n# Get the feature embedding of the training data\nwith torch.no_grad():\n    feature_embedding = dkl_model.feature_extractor(train_x)\n\n\n# Create a subplot with 1 row and 2 columns\nfig = make_subplots(rows=1, cols=2, subplot_titles=('Original Training Data', 'Feature Embedding'))\n\n# Add the original training data to the first subplot\nfig.add_trace(\n    go.Scatter(\n        x=train_x[:, 0].numpy(), \n        y=train_x[:, 1].numpy(), \n        mode='markers',\n        marker=dict(size=5, color=train_y.numpy(), colorscale='Viridis', opacity=0.8),\n        name='Training Data',\n        hoverinfo='text',\n        text=[f'Index: {i}' for i in range(len(train_x))]), # Custom hover text\n    row=1, col=1\n)\n\n# Add the feature embedding data to the second subplot\nfig.add_trace(\n    go.Scatter(\n        x=feature_embedding[:, 0].numpy(), \n        y=feature_embedding[:, 1].numpy(), \n        mode='markers',\n        marker=dict(size=5, color=train_y.numpy(), colorscale='Viridis', opacity=0.8),\n        name='Feature Embedding',\n        hoverinfo='text',\n        text=[f'Index: {i}' for i in range(len(feature_embedding))]), # Custom hover text\n    row=1, col=2\n)\n\n# Update layout for a cohesive look\nfig.update_layout(height=350, width=700, showlegend=False)\nfig.show()\n\n\n                                                \n\n\nIt is extremely interesting that the DNN has noticed the kind of simmetry of the McCormick function. As a result, it has learned an almost 1D representation of the data, ordered by the value of the function. Now, the GP can easily fit the data, as the feature space is more suitable for the task."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Andrea Ruglioni",
    "section": "",
    "text": "Welcome to my personal website! I’m a PhD student at the Swiss Finance Institute, EPFL.\nHere, you can find my latest blog posts, publications, teaching experience, and more. Feel free to reach out if you have any questions or comments.\nLearn more about me →"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Andrea Ruglioni",
    "section": "Publications →",
    "text": "Publications →\n\n\n\n\n\nOptimizing wave energy converters: A co-design approach using surrogate modeling\n\n\nA joint optimization of the physical structure and control system of wave energy converters using deep kernel learning as a surrogate model\n\n\n\n\n\nAndrea Ruglioni, Edoardo Pasta, Nicolàs Faedo, Paolo Brandimarte\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#blog",
    "href": "index.html#blog",
    "title": "Andrea Ruglioni",
    "section": "Blog →",
    "text": "Blog →\n\n\n\n\n\n\n\n\n\n\nDeep kernel learning\n\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-task Gaussian processes\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Gaussian processes\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/WEC/index.html",
    "href": "publications/WEC/index.html",
    "title": "Optimizing wave energy converters: A co-design approach using surrogate modeling",
    "section": "",
    "text": "Wave energy converters (WECs) offer a promising source of renewable energy, but they require a careful joint optimization of both their geometry andcontrol strategy in order to cope with the complexity of marine environments. This joint optimization process is hindered by high computational costs. This paper presents an integrated co-design framework that simultaneously optimizes the WEC physical structure and control system using deep kernel learning (DKL) as a surrogate model. The DKL model, trained on high-fidelity simulations, enables efficient and accurate predictions, while maintaining essential physical constraints. A case study on a spherical WEC shows significant improvements in computational efficiency and a suitable prediction accuracy, advancing WEC design methodologies."
  },
  {
    "objectID": "publications/WEC/index.html#abstract",
    "href": "publications/WEC/index.html#abstract",
    "title": "Optimizing wave energy converters: A co-design approach using surrogate modeling",
    "section": "",
    "text": "Wave energy converters (WECs) offer a promising source of renewable energy, but they require a careful joint optimization of both their geometry andcontrol strategy in order to cope with the complexity of marine environments. This joint optimization process is hindered by high computational costs. This paper presents an integrated co-design framework that simultaneously optimizes the WEC physical structure and control system using deep kernel learning (DKL) as a surrogate model. The DKL model, trained on high-fidelity simulations, enables efficient and accurate predictions, while maintaining essential physical constraints. A case study on a spherical WEC shows significant improvements in computational efficiency and a suitable prediction accuracy, advancing WEC design methodologies."
  }
]