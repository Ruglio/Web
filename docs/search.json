[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/gaussian_processes/gp.html",
    "href": "posts/gaussian_processes/gp.html",
    "title": "Introduction to Gaussian Processes",
    "section": "",
    "text": "Welcome to the first installment of our series on deep kernel learning. In this post, we’ll delve into Gaussian processes (GPs) and their application as regressors. We’ll start by exploring what Gaussian processes are and why they are powerful tools for regression tasks. In subsequent posts, we’ll build on this foundation to discuss multi-task Gaussian processes and how they can be combined with neural networks to create deep kernel models."
  },
  {
    "objectID": "posts/gaussian_processes/gp.html#abstract",
    "href": "posts/gaussian_processes/gp.html#abstract",
    "title": "Introduction to Gaussian Processes",
    "section": "",
    "text": "This article introduces Gaussian Processes (GPs), discussing their mathematical foundations and practical applications in regression tasks. Python code examples demonstrate their implementation with squared exponential and Matérn kernels, including handling noisy measurements."
  },
  {
    "objectID": "posts/gaussian_processes/gp.html#introduction",
    "href": "posts/gaussian_processes/gp.html#introduction",
    "title": "Introduction to Gaussian Processes",
    "section": "",
    "text": "Welcome to the first installment of our series on deep kernel learning. In this post, we’ll delve into Gaussian processes (GPs) and their application as regressors. We’ll start by exploring what Gaussian processes are and why they are powerful tools for regression tasks. In subsequent posts, we’ll build on this foundation to discuss multi-task Gaussian processes and how they can be combined with neural networks to create deep kernel models."
  },
  {
    "objectID": "posts/gaussian_processes/gp.html#gaussian-processes",
    "href": "posts/gaussian_processes/gp.html#gaussian-processes",
    "title": "Introduction to Gaussian Processes",
    "section": "Gaussian Processes",
    "text": "Gaussian Processes\nTo understand Gaussian Processes fully, it’s important to briefly mention the Kolmogorov Extension Theorem. This theorem guarantees the existence of a stochastic process, i.e., a collection of random variables \\(\\{Y_x\\}_{x \\in \\mathcal{X}}\\), that satisfies specified finite-dimensional distributions. In simpler terms, it ensures that we can define a Gaussian process by specifying its mean and covariance functions for any finite set of points. Therefore, similar to a multivariate Gaussian distribution, a Gaussian process is defined by its mean function \\(m(\\cdot) : \\mathcal{X} \\to \\mathbb{R}\\) and covariance function \\(k(\\cdot, \\cdot) : \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}\\).\n\\[\nf \\sim GP(m, k).\n\\]\nFor a set of input points \\(X = \\{x_1, x_2, \\ldots, x_n\\}\\), with observations \\(y = \\{y_1 = f(x_1), y_2 = f(x_2), \\ldots, y_n = f(x_n)\\}\\), the joint distribution of the observed outputs \\(y\\) and the function values at \\(X\\) is given by:\n\\[\n\\begin{pmatrix}\ny_1 \\\\\n\\vdots \\\\\ny_n\n\\end{pmatrix} \\sim \\mathcal{N}\\left(\n\\mathbf{m} =\n\\begin{pmatrix}\nm(x_1) \\\\\n\\vdots \\\\\nm(x_n)\n\\end{pmatrix},\n\\mathbf{K} =\n\\begin{pmatrix}\nk(x_1, x_1) & \\dots & k(x_1, x_n) \\\\\n\\vdots & \\ddots & \\vdots \\\\\nk(x_n, x_1) & \\dots & k(x_n, x_n)\n\\end{pmatrix}\\right)\n\\]\nThe mean function (often assumed to be zero) and \\(k\\) is the covariance function or kernel. The choice of kernel function is crucial as it defines the properties of the functions that can be represented. It defines the shape and smoothness of the functions sampled from the GP. Common choices include the squared exponential kernel and the Matérn kernel.\nObserve that, in a similar matter, we could define a t-student process, etc.\n\nMaking Predictions\nTo make predictions at new input points \\(x_*\\), we use the joint distribution of the observed outputs \\(y\\) and the function values at \\(x_*\\), which is given by:\n\\[\n\\begin{pmatrix}\ny \\\\\nf(x_*)\n\\end{pmatrix} \\sim \\mathcal{N}\\left(\n\\begin{pmatrix}\n\\mathbf{m} \\\\\n\\mu(x_*)\n\\end{pmatrix},\n\\begin{pmatrix}\n\\mathbf{K} & k(x, x_*) \\\\\nk(x_*, x) & k(x_*, x_*)\n\\end{pmatrix}\\right)\n\\]\nwhere \\(k(x_*, x)\\) is the covariance between the observed data and the new inputs.\nThe conditional distribution of \\(f(x_*)\\) given \\(y\\) is then Gaussian with mean and covariance: \\[\n\\mu(x_*) = k(x_*, x) \\mathbf{K}^{-1} (y - \\mathbf{m})\n\\] \\[\n\\sigma^2(x_*) = k(x_*, x_*) - k(x_*, x) \\mathbf{K}^{-1} k(x, x_*)\n\\]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Introduction to Gaussian Processes\n\n\n\n\n\n\nmachine-learning\n\n\n\nAn introduction to Gaussian Processes, discussing their mathematical foundations and practical applications in regression tasks.\n\n\n\n\n\nJun 25, 2024\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 25, 2024\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJun 22, 2024\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(0, 2*np.pi)\ny = np.sin(x)\n\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: sine plot\n\n\n\n\n\nFigure 1 is a plot"
  },
  {
    "objectID": "posts/gaussian_processes/gp.html#interactive-visualizations",
    "href": "posts/gaussian_processes/gp.html#interactive-visualizations",
    "title": "Introduction to Gaussian Processes",
    "section": "Interactive Visualizations",
    "text": "Interactive Visualizations\nLet’s explore some interactive plots to better understand how different parameters of the kernel functions influence the Gaussian process model.\n\nSquared Exponential Kernel\nThe squared exponential kernel (also known as the RBF kernel) is defined as:\n\\[\nk_{\\text{Exp}}(x, x') = \\sigma^2 \\exp \\left( -\\frac{(x - x')^2}{2l^2} \\right)\n\\]\nwhere \\(\\sigma^2\\) is the variance and \\(l\\) is the length scale. Below is an interactive plot that shows how the squared exponential kernel depends on the lengthscale and variance. Notice that with a small length scale, the function is more wiggly, while with a large length scale, it is smoother, as the kernel function decays more slowly with distance. Instead, the variance controls the amplitude of the function, with higher values leading to more variability.\n#| standalone: true\n#| viewerHeight: 475\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\nfrom shiny import App, render, ui\n\n\n# Define the kernel function\ndef exponential_quadratic_kernel(x1, x2, l=1.0, sigma2=1.0):\n    \"\"\"Computes the exponential quadratic kernel (squared exponential kernel).\"\"\"\n    sqdist = np.sum(x1**2, 1).reshape(-1, 1) + np.sum(x2**2, 1) - 2 * np.dot(x1, x2.T)\n    return sigma2 * np.exp(-0.5 / l**2 * sqdist)\n\n# Define the input space\nX = np.linspace(-4, 4, 100).reshape(-1, 1)\n\ndef plot_kernel_and_samples(l, sigma2):\n    # Compute the kernel matrix\n    K = exponential_quadratic_kernel(X, X, l=l, sigma2=sigma2)\n\n    # Create the plot\n    fig, ax = plt.subplots(2, 1, figsize=(18, 6), sharex=True)\n\n    # Plot the kernel\n    ax[0].plot(X, exponential_quadratic_kernel(X, np.zeros((1,1)), l=l, sigma2=sigma2))\n    ax[0].set_title(f\"Squared exponential kernel function\")\n\n    # Sample 5 functions from the Gaussian process defined by the kernel\n    mean = np.zeros(100)\n    cov = K\n    samples = multivariate_normal.rvs(mean, cov, 5)\n\n    # Plot the samples\n    for i in range(5):\n        ax[1].plot(X, samples[i])\n    ax[1].set_title(\"Samples from the GP\")\n    ax[1].set_xlabel(\"x\")\n\n    plt.tight_layout()\n    return fig\n\napp_ui = ui.page_fluid(\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_slider(\"length_scale\", \"Length Scale (l):\", min=0.1, max=5.0, value=1.0, step=0.1),\n            ui.input_slider(\"variance\", \"Variance (σ²):\", min=0.1, max=5.0, value=1.0, step=0.1)\n        ),\n        ui.panel_main(\n            ui.output_plot(\"kernelPlot\")\n        )\n    )\n)\n\ndef server(input, output, session):\n    @output\n    @render.plot\n    def kernelPlot():\n        l = input.length_scale()\n        sigma2 = input.variance()\n        fig = plot_kernel_and_samples(l, sigma2)\n        return fig\n\napp = App(app_ui, server)\n\n\nMatérn kernel\nThe Matérn kernel is defined as:\n\\[\nk_{\\text{Matérn}}(x, x') = \\sigma^2 \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left( \\sqrt{2\\nu} \\frac{|x - x'|}{l} \\right)^{\\nu} K_{\\nu} \\left( \\sqrt{2\\nu} \\frac{|x - x'|}{l} \\right)\n\\]\nwhere \\(\\nu\\) controls the smoothness of the function, \\(l\\) is the length scale, \\(\\sigma^2\\) is the variance, and \\(K_{\\nu}\\) is the modified Bessel function. Below is an interactive plot that shows how the Matérn kernel depends on the lengthscale, variance, and \\(\\nu\\). The former two parameters have the same effect as in the squared exponential kernel, while \\(\\nu\\) controls the smoothness of the function. Indeed, we have that the samples generated have smoothness \\(\\lceil \\nu \\rceil - 1\\), and for \\(\\nu \\to \\infty\\), the Matérn kernel converges to the squared exponential kernel.\n#| standalone: true\n#| viewerHeight: 475\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\nfrom shiny import App, render, ui\nfrom scipy.special import kv, gamma\nfrom scipy.spatial.distance import cdist\n\n\n# Define the kernel function\ndef matern_kernel(x1, x2, l=1.0, sigma2=1.0, nu=1.5):\n    \"\"\"Computes the Matérn kernel.\"\"\"\n    D = cdist(x1, x2, 'euclidean')\n    const = (2**(1-nu))/gamma(nu)\n    K = const * (np.sqrt(2*nu)*D/l)**nu * kv(nu, np.sqrt(2*nu)*D/l)\n    # Replace NaN values with 1 for x == x'\n    K[np.isnan(K)] = 1\n    K *= sigma2\n    return K\n\n# Define the input space\nX = np.linspace(-4, 4, 100).reshape(-1, 1)\n\ndef plot_kernel_and_samples(l, sigma2, nu):\n    # Compute the kernel matrix\n    K = matern_kernel(X, X, l=l, sigma2=sigma2, nu=nu)\n\n    # Create the plot\n    fig, ax = plt.subplots(2, 1, figsize=(18, 6), sharex=True)\n\n    # Plot the kernel\n    ax[0].plot(X, matern_kernel(X, np.zeros((1, 1)), l=l, sigma2=sigma2, nu=nu))\n    ax[0].set_title(f\"Matern kernel function\")\n\n    # Sample 5 functions from the Gaussian process defined by the kernel\n    mean = np.zeros(100)\n    cov = K\n    samples = multivariate_normal.rvs(mean, cov, 5)\n\n    # Plot the samples\n    for i in range(5):\n        ax[1].plot(X, samples[i])\n    ax[1].set_title(\"Samples from the GP\")\n    ax[1].set_xlabel(\"x\")\n\n    plt.tight_layout()\n    return fig\n\napp_ui = ui.page_fluid(\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_slider(\"length_scale\", \"Length Scale (l):\", min=0.1, max=5.0, value=1.0, step=0.1),\n            ui.input_slider(\"variance\", \"Variance (σ²):\", min=0.1, max=5.0, value=1.0, step=0.1),\n            ui.input_slider(\"smoothness_param\", \"Smoothness param (v):\", min=1.5, max=5.0, value=1.5, step=0.5)\n        ),\n        ui.panel_main(\n            ui.output_plot(\"kernelPlot\")\n        )\n    )\n)\n\ndef server(input, output, session):\n    @output\n    @render.plot\n    def kernelPlot():\n        l = input.length_scale()\n        sigma2 = input.variance()\n        nu = input.smoothness_param()\n        fig = plot_kernel_and_samples(l, sigma2, nu)\n        return fig\n\napp = App(app_ui, server)\n\n\nNoisy Observations\nNoise and measurement error are inevitable in real-world data, which can significantly impact the performance and reliability of predictive models. We can represent noisy observations as:\n\\[\ny = f(x) + \\epsilon\n\\]\nwhere \\(\\epsilon\\) is a random variable representing the noise. Usually, we assume that \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma_n^2)\\), where \\(\\sigma_n^2\\) is the variance of the noise.\nBelow is an interactive plot that demonstrates how noise influences the Gaussian process model. The plot shows the noisy training data (black points), from a noisy version of the true function \\(f(x) = \\sin(x)\\), the black line. The plot also shows the GP mean prediction (blue line) for the squared exponential and Matérn kernels, along with the 95% confidence intervals. For \\(\\sigma_n^2 = 0\\), the model perfectly interpolates the training data, while for higher noise levels, the model becomes less certain about the underlying function and provides wider confidence intervals.\n#| standalone: true\n#| viewerHeight: 475\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\nfrom shiny import App, render, ui\nfrom scipy.spatial.distance import cdist\nfrom scipy.special import kv, gamma\n\n# Define the kernel functions\ndef exponential_quadratic_kernel(x1, x2, l=1.0, sigma2=1.0):\n    \"\"\"Computes the exponential quadratic kernel (squared exponential kernel).\"\"\"\n    sqdist = np.sum(x1**2, 1).reshape(-1, 1) + np.sum(x2**2, 1) - 2 * np.dot(x1, x2.T)\n    return sigma2 * np.exp(-0.5 / l**2 * sqdist)\n\ndef matern_kernel(x1, x2, l=1.0, sigma2=1.0, nu=1.5):\n    \"\"\"Computes the Matérn kernel.\"\"\"\n    D = cdist(x1, x2, 'euclidean')\n    const = (2**(1-nu))/gamma(nu)\n    K = sigma2 * const * (np.sqrt(2*nu)*D/l)**nu * kv(nu, np.sqrt(2*nu)*D/l)\n    # Replace NaN values with 1 for x == x'\n    K[np.isnan(K)] = 1\n    return K\n\n# Define the input space\nX = np.linspace(-4, 4, 100).reshape(-1, 1)\n\ndef plot_kernels_and_samples(noise_level):\n    # Generate training data with noise\n    X_train = np.array([-3, -2, -1, 1, 3.5]).reshape(-1, 1)\n    y_train = np.sin(X_train) + noise_level * np.random.randn(X_train.shape[0], 1).reshape(-1,1)\n\n    # Compute the kernel matrices for training data\n    K_train_exp = exponential_quadratic_kernel(X_train, X_train) + noise_level**2 * np.eye(len(X_train))\n    K_s_exp = exponential_quadratic_kernel(X_train, X)\n    K_ss_exp = exponential_quadratic_kernel(X, X)\n    \n    K_train_matern = matern_kernel(X_train, X_train) + noise_level**2 * np.eye(len(X_train))\n    K_s_matern = matern_kernel(X_train, X)\n    K_ss_matern = matern_kernel(X, X)\n\n    # Compute the mean and covariance of the posterior distribution for exponential kernel\n    K_train_inv_exp = np.linalg.inv(K_train_exp)\n    mu_s_exp = (K_s_exp.T.dot(K_train_inv_exp).dot(y_train)).reshape(-1)\n    cov_s_exp = K_ss_exp - K_s_exp.T.dot(K_train_inv_exp).dot(K_s_exp)\n\n    # Compute the mean and covariance of the posterior distribution for Matérn kernel\n    K_train_inv_matern = np.linalg.inv(K_train_matern)\n    mu_s_matern = (K_s_matern.T.dot(K_train_inv_matern).dot(y_train)).reshape(-1)\n    cov_s_matern = K_ss_matern - K_s_matern.T.dot(K_train_inv_matern).dot(K_s_matern)\n\n    # Sample 5 functions from the posterior distribution for both kernels\n    samples_exp = multivariate_normal.rvs(mu_s_exp, cov_s_exp, 5)\n    samples_matern = multivariate_normal.rvs(mu_s_matern, cov_s_matern, 5)\n\n    # Create the plot\n    fig, ax = plt.subplots(2, 1, figsize=(18, 6), sharex=True)\n\n    # Plot the training data and GP predictions for exponential kernel\n    ax[0].scatter(X_train, y_train, color='black', zorder=10, label='Noisy observations')\n    ax[0].plot(X, mu_s_exp, color='blue', label='Mean')\n    ax[0].plot(X, np.sin(X), color='black', label='True function')\n    ax[0].fill_between(X.ravel(), mu_s_exp - 1.96 * np.sqrt(np.diag(cov_s_exp)), mu_s_exp + 1.96 * np.sqrt(np.diag(cov_s_exp)), color=\"blue\", alpha=0.2, label='95% confidence interval')\n    # for i in range(5):\n    #     ax[0].plot(X, samples_exp[i], alpha=0.5, linestyle='--')\n    ax[0].set_title(f\"Squared exponential kernel\")\n    # ax[0].legend()\n\n    # Plot the training data and GP predictions for Matérn kernel\n    ax[1].scatter(X_train, y_train, color='black', zorder=10, label='Noisy observations')\n    ax[1].plot(X, mu_s_matern, color=\"blue\", label='Mean')\n    ax[1].plot(X, np.sin(X), color=\"black\", label='True function')\n    ax[1].fill_between(X.ravel(), mu_s_matern - 1.96 * np.sqrt(np.diag(cov_s_matern)), mu_s_matern + 1.96 * np.sqrt(np.diag(cov_s_matern)), color=\"blue\", alpha=0.2, label='95% confidence interval')\n    # for i in range(5):\n    #     ax[1].plot(X, samples_matern[i], alpha=0.5, linestyle='--')\n    ax[1].set_title(f\"Matérn kernel\")\n    # ax[1].legend()\n    ax[1].set_xlabel(\"x\")\n\n    plt.tight_layout()\n    return fig\n\napp_ui = ui.page_fluid(\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_slider(\"noise_level\", \"Noise Level (σ²ₙ):\", min=0.0, max=1.0, value=0.0, step=0.01)\n        ),\n        ui.panel_main(\n            ui.output_plot(\"kernelPlot\")\n        )\n    )\n)\n\ndef server(input, output, session):\n    @output\n    @render.plot\n    def kernelPlot():\n        noise_level = input.noise_level()\n        fig = plot_kernels_and_samples(noise_level)\n        return fig\n\napp = App(app_ui, server)"
  },
  {
    "objectID": "posts/gaussian_processes/gp.html#applications",
    "href": "posts/gaussian_processes/gp.html#applications",
    "title": "Introduction to Gaussian Processes",
    "section": "Applications",
    "text": "Applications\n\nBayesian hyperparameter tuning\nHyperparameter tuning is a critical yet challenging aspect of training neural networks. Finding the optimal combination of hyperparameters, such as learning rate, batch size, number of layers, and units per layer, can significantly enhance a model’s performance. Traditional methods like grid search and random search often prove to be inefficient and computationally expensive. This is where Bayesian optimization, powered by Gaussian processes, comes into play, offering a smarter approach to hyperparameter tuning.\nUnlike exhaustive search methods, Bayesian optimization is more sample-efficient, meaning it can find optimal hyperparameters with fewer iterations. It works by\n\nmodeling the objective function (e.g., validation loss) as a Gaussian process in the hyperparameter space\nusing an acquisition function to decide where to sample next. The acquisition function balances exploration (sampling in unexplored regions) and exploitation (sampling in regions with low loss) to guide the search towards the global optimum.\n\n\n\nTime series forecasting\nGaussian processes are also widely used in time series forecasting due to their flexibility and ability to model complex patterns in the data. By treating time series data as a function of time, Gaussian processes can capture the underlying dynamics and dependencies in the series. They can provide not only point estimates but also probabilistic forecasts, including prediction intervals that quantify uncertainty."
  }
]