{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Deep kernel learning\"\n",
        "author: \"Andrea Ruglioni\"\n",
        "date: \"08 July 2024\"\n",
        "last-modified: \"09 July 2024\"\n",
        "description: \"\"\n",
        "tags: [machine-learning, gaussian-processes, regression, kernels, multi-output, multi-task, coregionalization, gpytorch, python, tutorial, code, example, deep kernel learning, dkl, deep gaussian processes, dgp, deep learning, neural networks, deep neural networks]\n",
        "categories: [machine-learning, deep-learning, gaussian-processes]\n",
        "image: \"mgp.png\"\n",
        "toc: true\n",
        "format: \n",
        "  html:\n",
        "    code-fold: true\n",
        "nocite: |\n",
        "  @*\n",
        "bibliography: refs.bib\n",
        "---\n",
        "\n",
        "\n",
        "In this post, we explore Deep Kernel Learning (DKL), a hybrid approach that combines the strengths of deep neural networks (DNNs) with Gaussian Processes (GPs). DKL offers a powerful framework for modeling complex data patterns, enhancing the predictive capabilities and interpretability of standard GPs.\n",
        "\n",
        "## Gaussian Processes (GPs)\n",
        "\n",
        "Gaussian Processes are non-parametric models used for regression and classification tasks. A GP is defined by its mean function \\( \\mu(\\mathbf{x}) \\) and covariance function (kernel) \\( k(\\mathbf{x}, \\mathbf{x}') \\).\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) \\sim \\mathcal{GP}(\\mu(\\mathbf{x}), k(\\mathbf{x}, \\mathbf{x}'))\n",
        "$$\n",
        "\n",
        "Given training data \\( \\mathbf{X} = [\\mathbf{x}_1, \\ldots, \\mathbf{x}_n] \\) and targets \\( \\mathbf{y} \\), the predictive distribution for a test point \\( \\mathbf{x}_* \\) is:\n",
        "\n",
        "$$\n",
        "p(f_* | \\mathbf{X}, \\mathbf{y}, \\mathbf{x}_*) = \\mathcal{N}(\\mathbf{k}_*^\\top (\\mathbf{K} + \\sigma^2 \\mathbf{I})^{-1} \\mathbf{y}, k(\\mathbf{x}_*, \\mathbf{x}_*) - \\mathbf{k}_*^\\top (\\mathbf{K} + \\sigma^2 \\mathbf{I})^{-1} \\mathbf{k}_*)\n",
        "$$\n",
        "\n",
        "where \\( \\mathbf{K} \\) is the kernel matrix for training data, and \\( \\mathbf{k}_* \\) is the covariance vector between \\( \\mathbf{x}_* \\) and training points.\n",
        "\n",
        "## Deep Kernel Learning (DKL)\n",
        "\n",
        "DKL integrates DNNs with GPs by using a DNN to learn a representation of the data, which is then used to define the GP kernel. This allows the GP to capture complex patterns in the data that a standard kernel might miss.\n",
        "\n",
        "### Theory\n",
        "\n",
        "In DKL, the DNN acts as a feature extractor, transforming input \\( \\mathbf{x} \\) into a feature vector \\( \\mathbf{z} = \\phi(\\mathbf{x}) \\). The GP kernel is then defined on these features:\n",
        "\n",
        "$$\n",
        "k(\\mathbf{x}, \\mathbf{x}') = k(\\phi(\\mathbf{x}), \\phi(\\mathbf{x}'))\n",
        "$$\n",
        "\n",
        "The DNN parameters \\( \\theta \\) and the GP hyperparameters are jointly optimized during training.\n",
        "\n",
        "### Implementation with GPyTorch\n",
        "\n",
        "Let's implement DKL using GPyTorch.\n"
      ],
      "id": "ddb17dcd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pip install plotly"
      ],
      "id": "f873e7ac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import gpytorch\n",
        "import plotly.graph_objects as go\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the McCormick function\n",
        "def mccormick(x1, x2):\n",
        "    return np.sin(x1 + x2) + (x1 - x2)**2 - 1.5 * x1 + 2.5 * x2 + 1\n",
        "\n",
        "# Generate grid data for plotting\n",
        "x1 = np.linspace(-1.5, 4, 100)\n",
        "x2 = np.linspace(-3, 4, 100)\n",
        "x1_test, x2_test = np.meshgrid(x1, x2)\n",
        "y = mccormick(x1_test, x2_test)\n",
        "\n",
        "# Generate test data\n",
        "test_x = torch.tensor(np.vstack((x1_test.ravel(), x2_test.ravel())).T, dtype=torch.float32)\n",
        "test_y = torch.tensor(y.ravel(), dtype=torch.float32)\n",
        "\n",
        "# Generate training data\n",
        "x1 = np.linspace(-1.5, 4, 5)\n",
        "x2 = np.linspace(-3, 4, 5)\n",
        "x1_train, x2_train = np.meshgrid(x1, x2)\n",
        "train_x = np.vstack((x1_train.ravel(), x2_train.ravel())).T\n",
        "train_y = mccormick(train_x[:, 0], train_x[:, 1])\n",
        "\n",
        "# Convert training data to tensors\n",
        "train_x = torch.tensor(train_x, dtype=torch.float32)\n",
        "train_y = torch.tensor(train_y.ravel(), dtype=torch.float32)\n",
        "\n",
        "# Define the DNN feature extractor\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 4)\n",
        "        self.fc2 = nn.Linear(4, 8)\n",
        "        self.fc3 = nn.Linear(8, 2)\n",
        "        self.activation = nn.Softplus()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Define the DKL model\n",
        "class DKLModel(gpytorch.models.ExactGP):\n",
        "    def __init__(self, train_x, train_y, likelihood):\n",
        "        super(DKLModel, self).__init__(train_x, train_y, likelihood)\n",
        "        feature_extractor = FeatureExtractor()\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.num_dims = 2\n",
        "        self.mean_module = gpytorch.means.ConstantMean()\n",
        "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
        "    \n",
        "    def forward(self, x):\n",
        "        projected_x = self.feature_extractor(x)\n",
        "        mean_x = self.mean_module(projected_x)\n",
        "        covar_x = self.covar_module(projected_x)\n",
        "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "\n",
        "# Define the GP model\n",
        "class GPModel(gpytorch.models.ExactGP):\n",
        "    def __init__(self, train_x, train_y, likelihood):\n",
        "        super(GPModel, self).__init__(train_x, train_y, likelihood)\n",
        "        self.mean_module = gpytorch.means.ConstantMean()\n",
        "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x)\n",
        "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "\n",
        "# Initialize likelihood and models\n",
        "dkl_likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
        "dkl_model = DKLModel(train_x, train_y, dkl_likelihood)\n",
        "gp_likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
        "gp_model = GPModel(train_x, train_y, gp_likelihood)\n",
        "\n",
        "# Training parameters\n",
        "training_iterations = 500\n",
        "\n",
        "# Train DKL model\n",
        "dkl_model.train()\n",
        "dkl_likelihood.train()\n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': dkl_model.feature_extractor.parameters()},\n",
        "    {'params': dkl_model.covar_module.parameters()},\n",
        "    {'params': dkl_model.mean_module.parameters()},\n",
        "    {'params': dkl_likelihood.parameters()},\n",
        "], lr=0.1)\n",
        "mll = gpytorch.mlls.ExactMarginalLogLikelihood(dkl_likelihood, dkl_model)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
        "\n",
        "for i in range(training_iterations):\n",
        "    optimizer.zero_grad()\n",
        "    output = dkl_model(train_x)\n",
        "    loss = -mll(output, train_y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "# Train GP model\n",
        "gp_model.train()\n",
        "gp_likelihood.train()\n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': gp_model.covar_module.parameters()},\n",
        "    {'params': gp_model.mean_module.parameters()},\n",
        "    {'params': gp_likelihood.parameters()},\n",
        "], lr=0.1)\n",
        "mll = gpytorch.mlls.ExactMarginalLogLikelihood(gp_likelihood, gp_model)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
        "\n",
        "for i in range(training_iterations):\n",
        "    optimizer.zero_grad()\n",
        "    output = gp_model(train_x)\n",
        "    loss = -mll(output, train_y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "# Evaluate models\n",
        "dkl_model.eval()\n",
        "dkl_likelihood.eval()\n",
        "gp_model.eval()\n",
        "gp_likelihood.eval()\n",
        "\n",
        "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
        "    dkl_pred = dkl_likelihood(dkl_model(test_x))\n",
        "    gp_pred = gp_likelihood(gp_model(test_x))\n",
        "\n",
        "dkl_pred = dkl_pred.mean.numpy().reshape(x1_test.shape)\n",
        "gp_pred = gp_pred.mean.numpy().reshape(x1_test.shape)\n",
        "true_y = test_y.numpy().reshape(x1_test.shape)\n",
        "\n",
        "# Create interactive 3D plots\n",
        "fig = go.Figure()\n",
        "\n",
        "# DKL predictions surface\n",
        "fig.add_trace(go.Surface(z=dkl_pred, x=x1_test, y=x2_test, colorscale='Blues', opacity=0.8, name='DKL', showscale=False))\n",
        "\n",
        "# GP predictions surface\n",
        "fig.add_trace(go.Surface(z=gp_pred, x=x1_test, y=x2_test, colorscale='Greens', opacity=0.8, name='GP', showscale=False))\n",
        "\n",
        "# True surface\n",
        "fig.add_trace(go.Surface(z=true_y, x=x1_test, y=x2_test, colorscale='Reds', opacity=0.5, name='Function', showscale=False))\n",
        "\n",
        "# Add training points\n",
        "fig.add_trace(go.Scatter3d(x=train_x[:, 0].numpy(), y=train_x[:, 1].numpy(), z=train_y.numpy(), mode='markers', marker=dict(size=3, color='black', opacity=0.8), name='Train Points'))\n",
        "\n",
        "fig.update_layout(scene=dict(xaxis_title='X1', yaxis_title='X2', zaxis_title='Y'))\n",
        "\n",
        "fig.show()"
      ],
      "id": "78a007b8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison with Standard Gaussian Processes\n",
        "\n",
        "To illustrate the benefits of DKL, we compare its performance with a standard GP on a synthetic dataset. The DKL model captures more complex patterns and provides better uncertainty estimates.\n"
      ],
      "id": "dbba208c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# get the metrics\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "dkl_mse = mean_squared_error(test_y.numpy(), dkl_pred.ravel())\n",
        "gp_mse = mean_squared_error(test_y.numpy(), gp_pred.ravel())\n",
        "\n",
        "\n",
        "df = pd.DataFrame({'Model': ['DKL', 'GP'],\n",
        "                   'MSE': [dkl_mse, gp_mse])\n",
        "df"
      ],
      "id": "d10d2c1c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Concusion\n",
        "\n",
        "Deep Kernel Learning enhances the flexibility and predictive power of Gaussian Processes by leveraging the feature extraction capabilities of deep neural networks. This hybrid approach captures complex data patterns more effectively than standard GPs, making it a valuable tool for advanced machine learning tasks."
      ],
      "id": "08bf2002"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\rugli\\AppData\\Roaming\\Python\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}