{
  "hash": "48f9c6be8b60d36b24f159132e373856",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Multi-task Gaussian processes\"\nauthor: \"Andrea Ruglioni\"\ndate: \"08 July 2024\"\nlast-modified: \"09 July 2024\"\ndescription: \"Learn how to model multiple correlated outputs using the intrinsic model of coregionalization (ICM) and the linear model of coregionalization (LMC).\"\ntags: [machine-learning, gaussian-processes, regression, kernels, multi-output, multi-task, coregionalization, gpytorch, python, tutorial, code, example]\ncategories: [machine-learning, gaussian-processes]\nimage: \"mgp.png\"\ntoc: true\nformat: \n  html:\n    code-fold: true\nnocite: |\n  @*\nbibliography: refs.bib\n---\n\n\n\n\n[In my previous post on GPs](../s1/gp.qmd), we discussed the basics of GPs and their applications in regression tasks. Here, we extend the discussion to multi-task GPs, highlighting their benefits and practical implementations.\nWe will provide an intuitive explanation of the concepts and showcase code examples using `GPyTorch`.\nLet's dive in!\n\n## Understanding multi-task GPs\n\nGaussian processes are a powerful tool for regression and classification tasks, offering a non-parametric way to define distributions over functions. When dealing with multiple related outputs, a multi-task GP can model the dependencies between these tasks, leading to better generalization and predictions.\n\nMathematically, a Gaussian process is defined as a collection of random variables, any finite number of which have a joint Gaussian distribution. For a set of input points $X$, the corresponding output values $f(X)$ are jointly Gaussian:\n\n$$\nf(X) \\sim \\mathcal{N}(m(X), k(X, X))\n$$\n\nwhere $m(X)$ is the mean function and $k(X, X)$ is the covariance matrix.\n\nIn a multitask setting, we aim to model the function $f: \\mathcal{X} \\to \\mathbb{R}^T$, so that we have $T$ outputs, or tasks, $\\{f_t(X)\\}_{t=1}^T$. This means that the mean function is $m: \\mathcal{X} \\to \\mathbb{R}^T$ and the kernel function is $k: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}^{T \\times T}$.\nHow can we model the correlations between these tasks?\n\n\n### Independent multi-task GP\n\nA simple independent multioutput GP models each task independently, without considering any correlations between tasks. In this setup, each task has its own GP with its own mean and covariance functions. Mathematically, this can be expressed as:\n\n$$\nf_i(X) \\sim \\mathcal{N}(m_i(X), k_i(X, X)) \\qquad i = 1, \\ldots, T,\n$$\n\nleading to a block-diagonal covariance matrix $k(x, x) = \\text{diag}(k_1(x, x), \\ldots, k_T(x, x))$.\nThis approach does not leverage any shared information between tasks, which can lead to suboptimal performance, especially when there is limited data for some tasks.\n\n### Intrinsic model of coregionalization (ICM)\n\nThe ICM approach generalizes the independent multioutput GP by introducing a coregionalization matrix $B$ that models the correlations between tasks. Specifically, the covariance function in the ICM approach is defined as:\n\n$$\nk(x, x') = k_{\\text{input}}(x, x') B,\n$$\n\nwhere $k_{\\text{input}}$ is a covariance function defined over the input space (e.g. squared exponential kernel), and $B \\in \\mathbb{R}^{T \\times T}$ is the coregionalization matrix capturing the task-specific covariances. The matrix $B$ is typically parameterized as $B = W W^\\mathsf{T}$, with $W \\in \\mathbb{R}^{T \\times r}$ and $r$ being the rank of the coregionalization matrix.\nThis ensures the kernel is positive semi-definite.\n\nThe ICM approach can learn the shared structure between tasks.\nIndeed, the Pearson correlation coefficient between tasks can be expressed as:\n\n$$\n\\rho_{ij} = \\frac{B[i, j]}{\\sqrt{B[i, i] B[j, j]}}.\n$$\n\n\n\n### Linear model of coregionalization (LMC)\n\nAnother common approach is the LMC model, which extends the ICM by allowing for a wider variety of input kernels. In the LMC model, the covariance function is defined as:\n\n$$\nk(x, x') = \\sum_{q=1}^Q k_{\\text{input}}^{(q)}(x, x') B_q,\n$$\n\nwhere $Q$ is the number of base kernels, $k_{\\text{input}}^{(q)}$ are the base kernels, and $B_q$ are the coregionalization matrices for each base kernel. This model can capture even more complex correlations between tasks by combining multiple base kernels.\nWe can recover the ICM model by setting $Q=1$.\n\n## Noise modeling\n\nIn multi-task GPs, we have to consider a multi-output likelihood function that models the noise for each task.\nThe standard likelihood function is typically a multidimensional Gaussian likelihood, which can be expressed as:\n\n$$\ny = f(x) + \\epsilon, \\qquad \\epsilon \\sim \\mathcal{N}_T(0, \\Sigma),\n$$\n\nwhere $y$ is the observed output, $f(x)$ is the latent function, and $\\Sigma$ is the noise covariance matrix.\nThe flexibility is on the choice of the noise covariance matrix, which can be diagonal $\\Sigma = \\text{diag}(\\sigma_1^2, \\ldots, \\sigma_T^2)$ (independent noise for each task) or full (correlated noise across tasks).\nThe latter is usually represented as $\\Sigma = L L^\\mathsf{T}$, where $L \\in \\mathbb{R}^{T \\times r}$ and $r$ is the rank of the noise covariance matrix.\nThis allows for capturing correlations between the noise terms of different tasks.\n\nThe final covariance matrix with noise is then given by:\n\n$$\nK_n = K + \\mathbf{I} \\otimes \\Sigma,\n$$\n\nwhere $K$ is the covariance matrix without noise, $\\mathbf{I}$ is the identity matrix, and $\\otimes$ denotes the Kronecker product. The noise term is added to the diagonal blocks of the covariance matrix.\n\n## Implementation with `GPyTorch`\n\nLet's walk through an example of implementing a multitask GP using `GPyTorch` with the ICM kernel.\nFirst of all, we need to install the required packages, including `Torch`, `GPyTorch`, `matplotlib`, and `seaborn`, `numpy`.\n\n::: {#137373c9 .cell pip='true' execution_count=1}\n``` {.python .cell-code}\n%pip install torch gpytorch matplotlib seaborn numpy pandas\n\nimport torch\nimport gpytorch\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\n```\n:::\n\n\nAfterward, we can define the multitask GP model.\nWe use an ICM kernel (with rank $r=1$) to capture correlations between tasks.\nWe generate synthetic noisy training data for two tasks (sine and a shifted sine), so to have correlated outputs.\nThe noise covariance matrix is\n\n$$\n\\Sigma = \\begin{bmatrix}\n\\sigma_1^2 & \\rho \\sigma_1 \\sigma_2 \\\\\n\\rho \\sigma_1 \\sigma_2 & \\sigma_2^2\n\\end{bmatrix},\n$$\n\nwhere $\\sigma_1^2 = \\sigma_2^2 = 0.1^2$ and $\\rho = 0.3$.\n\nLastly, we train the model and evaluate its performance by plotting the mean predictions and confidence intervals for each task.\n\n\n\n::: {#cell-fig-multitaskgpicm .cell execution_count=3}\n``` {.python .cell-code}\n# Define the kernel with coregionalization\nclass MultitaskGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood, num_tasks):\n        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.MultitaskMean(\n            gpytorch.means.ConstantMean(), num_tasks=num_tasks\n        )\n        self.covar_module = gpytorch.kernels.MultitaskKernel(\n            gpytorch.kernels.RBFKernel(), num_tasks=num_tasks, rank=1\n        )\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n\n# Training data\nf1 = lambda x:  torch.sin(x * (2 * torch.pi))\nf2 = lambda x: torch.sin((x - 0.1) * (2 * torch.pi))\ntrain_x = torch.linspace(0, 1, 10)\ntrain_y = torch.stack([\n    f1(train_x),\n    f2(train_x)\n]).T\n# Define the noise covariance matrix with correlation = 0.3\nsigma2 = 0.1**2\nSigma = torch.tensor([[sigma2, 0.3 * sigma2], [0.3 * sigma2, sigma2]])\n# Add noise to the training data\ntrain_y += torch.tensor(np.random.multivariate_normal(mean=[0,0], cov=Sigma, size=len(train_x)))\n\n# Model and likelihood\nnum_tasks = 2\nlikelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=num_tasks, rank=1)\nmodel = MultitaskGPModel(train_x, train_y, likelihood, num_tasks)\n\n# Training the model\nmodel.train()\nlikelihood.train()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\nmll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n\nnum_iter = 500\nfor i in range(num_iter):\n    optimizer.zero_grad()\n    output = model(train_x)\n    loss = -mll(output, train_y)\n    loss.backward()\n    optimizer.step()\n    scheduler.step()\n\n# Evaluation\nmodel.eval()\nlikelihood.eval()\n\ntest_x = torch.linspace(0, 1, 100)\n\nwith torch.no_grad(), gpytorch.settings.fast_pred_var():\n    pred_multi = likelihood(model(test_x))\n\n# Plot predictions\nfig, ax = plt.subplots()\n\ncolors = ['blue', 'red']\nfor i in range(num_tasks):\n    ax.plot(test_x, pred_multi.mean[:, i], label=f'Mean prediction (Task {i+1})', color=colors[i])\n    ax.plot(test_x, [f1(test_x), f2(test_x)][i], linestyle='--', label=f'True function (Task {i+1})')\n    lower = pred_multi.confidence_region()[0][:, i].detach().numpy()\n    upper = pred_multi.confidence_region()[1][:, i].detach().numpy()\n    ax.fill_between(\n        test_x,\n        lower,\n        upper,\n        alpha=0.2,\n        label=f'Confidence interval (Task {i+1})',\n        color=colors[i]\n    )\n\nax.scatter(train_x, train_y[:, 0], color='black', label=f'Training data (Task 1)')\nax.scatter(train_x, train_y[:, 1], color='gray', label=f'Training data (Task 2)')\n\nax.set_title('Multitask GP with ICM')\nax.legend(loc='lower center', bbox_to_anchor=(0.5, -0.2),\n          ncol=3, fancybox=True)\n```\n\n::: {.cell-output .cell-output-display}\n![](mgp_files/figure-html/fig-multitaskgpicm-output-1.png){#fig-multitaskgpicm width=709 height=476 fig-align='center'}\n:::\n:::\n\n\nUsing `GPyTorch`, the ICM model is straightforward to implement by using the `MultitaskMean`, `MultitaskKernel`, and `MultitaskGaussianLikelihood` classes.\nThese take care of the multitask structure, the noise and coregionalization matrices, allowing us to focus on the model definition and training.\n\nRegarding the training loop, it works similarly to standard GPs, with the negative marginal log-likelihood as the loss function, and an optimizer to update the model parameters.\nA scheduler has been added to reduce the learning rate during training, which can help stabilize the optimization process.\n\n@fig-matrices show the coregionalization matrix $B$ learned by the model, and the noise covariance matrix $\\Sigma$.\nThe former captures the correlations between the tasks.\nAs we can see, the off-diagonal elements of $B$ are positive.\nThe latter represents the noise levels for each task. Notice that the model has properly learned the noise correlation.\n\n::: {#fig-matrices .cell layout-ncol='2' execution_count=4}\n``` {.python .cell-code}\nW = model.covar_module.task_covar_module.covar_factor\nB = W @ W.T\n\nfig, ax = plt.subplots()\nsns.heatmap(B.detach().numpy(), annot=True, ax=ax, cbar=False, square=True)\nax.set_xticklabels(['Task 1', 'Task 2'])\nax.set_yticklabels(['Task 1', 'Task 2'])\nax.set_title('Coregionalization matrix B')\nfig.show()\n\n\nL = model.likelihood.task_noise_covar_factor.detach().numpy()\nSigma = L @ L.T\n\nfig, ax = plt.subplots()\nsns.heatmap(Sigma, annot=True, ax=ax, cbar=False, square=True)\nax.set_xticklabels(['Task 1', 'Task 2'])\nax.set_yticklabels(['Task 1', 'Task 2'])\nax.set_title('Noise covariance matrix')\nfig.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](mgp_files/figure-html/fig-matrices-output-1.png){#fig-matrices-1 width=411 height=431 fig-align='center'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](mgp_files/figure-html/fig-matrices-output-2.png){#fig-matrices-2 width=411 height=431 fig-align='center'}\n:::\n:::\n\n\n### Comparison\n\nTo highlight the advantages of modeling correlated outputs using the ICM approach, let's compare it with a model that treats each task independently, ignoring any potential correlations between tasks.\nWe can define a separate GP for each task, train them, and evaluate their performance on the test data.\n\n::: {#cell-fig-independentgps .cell execution_count=5}\n``` {.python .cell-code}\nclass IndependentGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(IndependentGPModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n# Create models and likelihoods for each task\nlikelihoods = [gpytorch.likelihoods.GaussianLikelihood() for _ in range(num_tasks)]\nmodels = [IndependentGPModel(train_x, train_y[:, i], likelihoods[i]) for i in range(num_tasks)]\n\n# Training the independent models\nfor i, (model, likelihood) in enumerate(zip(models, likelihoods)):\n    model.train()\n    likelihood.train()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n    \n    for _ in range(num_iter):\n        optimizer.zero_grad()\n        output = model(train_x)\n        loss = -mll(output, train_y[:, i])\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n# Evaluation\nfor model, likelihood in zip(models, likelihoods):\n    model.eval()\n    likelihood.eval()\n\nwith torch.no_grad(), gpytorch.settings.fast_pred_var():\n    pred_inde = [likelihood(model(test_x)) for model, likelihood in zip(models, likelihoods)]\n\n# Plot predictions\nfig, ax = plt.subplots()\n\nfor i in range(num_tasks):\n    ax.plot(test_x, pred_inde[i].mean, label=f'Mean prediction (Task {i+1})', color=colors[i])\n    ax.plot(test_x, [f1(test_x), f2(test_x)][i], linestyle='--', label=f'True function (Task {i+1})')\n    lower = pred_inde[i].confidence_region()[0]\n    upper = pred_inde[i].confidence_region()[1]\n    ax.fill_between(\n        test_x,\n        lower,\n        upper,\n        alpha=0.2,\n        label=f'Confidence interval (Task {i+1})',\n        color=colors[i]\n    )\n\nax.scatter(train_x, train_y[:, 0], color='black', label='Training data (Task 1)')\nax.scatter(train_x, train_y[:, 1], color='gray', label='Training data (Task 2)')\n\nax.set_title('Independent GPs')\nax.legend(loc='lower center', bbox_to_anchor=(0.5, -0.2),\n          ncol=3, fancybox=True)\n```\n\n::: {.cell-output .cell-output-display}\n![](mgp_files/figure-html/fig-independentgps-output-1.png){#fig-independentgps width=709 height=476 fig-align='center'}\n:::\n:::\n\n\nIn terms of performance, we can compare the mean squared error (MSE) of the predictions on the test data for the multitask GP with ICM and the independent GPs.\n\n::: {#bd908590 .cell execution_count=6}\n``` {.python .cell-code}\nmean_multi = pred_multi.mean.numpy()\nmean_inde = np.stack([pred.mean.numpy() for pred in pred_inde]).T\n\ntest_y = torch.stack([f1(test_x), f2(test_x)]).T.numpy()\nMSE_multi = np.mean((mean_multi - test_y) ** 2)\nMSE_inde = np.mean((mean_inde - test_y) ** 2)\n\ndf = pd.DataFrame({\n    'Model': ['ICM', 'Independent'],\n    'MSE': [MSE_multi, MSE_inde]\n  })\ndf\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>MSE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ICM</td>\n      <td>0.002097</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Independent</td>\n      <td>0.002626</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe results show that ICM slightly outperforms the independent GPs in terms of MSE, thanks to the shared structure learned by the coregionalization matrix.\nIn practice, the improvement can be more significant when dealing with more complex tasks or limited data.\nIndeed, in the independent scenario, each GP learns from a smaller dataset of 10 points, potentially leading to overfitting or suboptimal generalization.\nOn the other hand, the multitask GP with ICM uses all the 20 points to learn the squared exponential kernel parameters. This shared information helps to improve the predictions for both tasks.\n\n",
    "supporting": [
      "mgp_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}