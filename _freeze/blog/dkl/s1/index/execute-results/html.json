{
  "hash": "633f6f3dadf469011a17c373de9403d6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Introduction to Gaussian processes\"\nauthor: \"Andrea Ruglioni\"\nimage: featured.png\ndate: \"25 June 2024\"\nlast-modified: \"07 July 2024\"\ndescription: \"An introduction to Gaussian Processes, discussing their mathematical foundations and practical applications in regression tasks.\"\ntags: [machine-learning, gaussian-processes, regression, kernels, code, examples, applications]\ncategories: [machine-learning, gaussian-processes]\n\nfilters:\n  - shinylive\nnocite: |\n  @*\nbibliography: refs.bib\njupyter: python3\n---\n\n\n\nWelcome to the first installment of our series on deep kernel learning. In this post, we'll delve into Gaussian processes (GPs) and their application as regressors.\nWe'll start by exploring what GPs are and why they are powerful tools for regression tasks.\nIn subsequent posts, we'll build on this foundation to discuss multi-task Gaussian processes and how they can be combined with neural networks to create deep kernel models.\n\n## Gaussian processes\n\nTo understand Gaussian processes fully, it's important to briefly mention the Kolmogorov extension theorem.\nThis theorem guarantees the existence of a stochastic process, i.e., a collection of random variables $\\{Y_x\\}_{x \\in \\mathcal{X}}, Y_x \\in \\mathbb{R}$, that satisfies a specified finite-dimensional distribution.\nFor instance, it ensures that we can define a Gaussian process by specyfing that any finite set of random variables has a multivariate Gaussian distribution, without worrying about the infinite-dimensional nature of the process.\nObserve that, in a similar matter, we could define a t-student process, by imposing that finite-dimensional distributions are t-student.\n\nTherefore, similar to a multivariate Gaussian distribution, a Gaussian process $f$ is defined by its mean function $m(\\cdot) : \\mathcal{X} \\to \\mathbb{R}$ and covariance function $k(\\cdot, \\cdot) : \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$:\n\n$$\nf \\sim GP(m, k),\n$$\n\nand it can be interpreted as an infinite-dimensional generalization of a multivariate Gaussian distribution.\n\nIn a regression setting, we could use $f$ as a surrogate model of a function $g: \\mathcal{X} \\to \\mathbb{R}$, where $\\mathcal{X}$ is the input space.\nSuppose you have a set of input points $X = \\{x_1, x_2, \\ldots, x_n\\}$, with observations $Y = \\{y_1 = g(x_1), y_2 = g(x_2), \\ldots, y_n = g(x_n)\\}$, the joint distribution of the observed outputs $Y$, assuming a GP prior, is given by:\n\n$$\n \\begin{pmatrix}\ny_1 \\\\\n\\vdots \\\\\ny_n\n\\end{pmatrix} \\sim \\mathcal{N}\\left(\n\\mathbf{m} = \n\\begin{pmatrix}\nm(x_1) \\\\\n\\vdots \\\\\nm(x_n)\n\\end{pmatrix},\n\\mathbf{K} = \n\\begin{pmatrix}\nk(x_1, x_1) & \\dots & k(x_1, x_n) \\\\\n\\vdots & \\ddots & \\vdots \\\\\nk(x_n, x_1) & \\dots & k(x_n, x_n)\n\\end{pmatrix}\\right),\n$$\n\nwhere $\\mathbf{m}$ is the vector of mean function values and $\\mathbf{K}$ is the covariance matrix of the function values at the input points.\nThis approach allows us to make predictions at new input points $x_*$ by conditioning on the observed data, providing not only point estimates but also uncertainty estimates.\n\n### Making predictions\n\nTo make a prediction $y_* = g(x_*)$ at new input point, we use the joint distribution of the observed outputs $Y$ and the function values at $x_*$, which is given by:\n\n$$\n \\begin{pmatrix}\nY \\\\\ny_*\n\\end{pmatrix} \\sim \\mathcal{N}\\left(\n\\begin{pmatrix}\n\\mathbf{m} \\\\\nm(x_*)\n\\end{pmatrix},\n\\begin{pmatrix}\n\\mathbf{K} & k(X, x_*) \\\\\nk(x_*, X) & k(x_*, x_*)\n\\end{pmatrix}\\right)\n$$\n\nwhere $k(x_*, X)$ is vector of covariances between the new input point $x_*$ and the observed data points $X$.\nThe conditional distribution of $y_*$ given $Y$ is then Gaussian with mean and covariance:\n\n$$\n\\mu(x_* \\mid X, Y) = k(x_*, X) \\mathbf{K}^{-1} (Y - \\mathbf{m}),\n$$\n$$\ns^2(x_* \\mid X, Y) = k(x_*, x_*) - k(x_*, X) \\mathbf{K}^{-1} k(X, x_*).\n$$\n\nTherefore, given the observed data, we can estimate the function value at a new input point $x_*$ as $\\mu(x_*)$ and quantify the uncertainty in the prediction as $s^2(x_*)$.\nThis is a key advantage of GPs, which can be important in decision-making processes.\n\n## Interactive visualizations\n\nLet's explore some interactive plots to better understand how the kernel functions influence the Gaussian process model.\nIndeed, the choice of the kernel function is crucial in defining the prior over functions, as it determines the smoothness and periodicity of the functions that the GP can model.\nTherefore, they play a fundamental role in the model's flexibility and generalization capabilities, and they can be tailored to the specific characteristics of the data at hand.\nOn  the other hand, the mean function is usually set constant, as the kernel is flexible enough.\n\n### Squared exponential kernel\n\nThe squared exponential kernel (also known as the RBF kernel) is defined as:\n\n$$\nk_{\\text{Exp}}(x, x') = \\sigma^2 \\exp \\left( -\\frac{(x - x')^2}{2l^2} \\right)\n$$\n\nwhere $\\sigma^2$ is the variance and $l$ is the length scale.\nBelow is an interactive plot that shows how the squared exponential kernel depends on the lengthscale and variance.\nNotice that with a small length scale, the function is more wiggly.\nInstead, with a large length scale it is smoother, as the kernel function decays more slowly with distance (i.e., the correlation between faraway points is higher, and they are more similar to each other).\nInstead, the variance controls the amplitude of the function, with higher values leading to more variability.\n\n\n```{shinylive-python}\n#| standalone: true\n#| viewerHeight: auto\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\nfrom shiny import App, render, ui\n\n\n# Define the kernel function\ndef exponential_quadratic_kernel(x1, x2, l=1.0, sigma2=1.0):\n    \"\"\"Computes the exponential quadratic kernel (squared exponential kernel).\"\"\"\n    sqdist = np.sum(x1**2, 1).reshape(-1, 1) + np.sum(x2**2, 1) - 2 * np.dot(x1, x2.T)\n    return sigma2 * np.exp(-0.5 / l**2 * sqdist)\n\n# Define the input space\nX = np.linspace(-4, 4, 100).reshape(-1, 1)\n\ndef plot_kernel_and_samples(l, sigma2):\n    # Compute the kernel matrix\n    K = exponential_quadratic_kernel(X, X, l=l, sigma2=sigma2)\n\n    # Create the plot\n    fig, ax = plt.subplots(2, 1, figsize=(18, 6), sharex=True)\n\n    # Plot the kernel\n    ax[0].plot(X, exponential_quadratic_kernel(X, np.zeros((1,1)), l=l, sigma2=sigma2))\n    ax[0].set_title(f\"Squared exponential kernel function\")\n\n    # Sample 5 functions from the Gaussian process defined by the kernel\n    mean = np.zeros(100)\n    cov = K\n    samples = multivariate_normal.rvs(mean, cov, 5)\n\n    # Plot the samples\n    for i in range(5):\n        ax[1].plot(X, samples[i])\n    ax[1].set_title(\"Samples from the GP\")\n    ax[1].set_xlabel(\"x\")\n\n    plt.tight_layout()\n    return fig\n\napp_ui = ui.page_fluid(\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_slider(\"length_scale\", \"Length Scale (l):\", min=0.1, max=5.0, value=1.0, step=0.1),\n            ui.input_slider(\"variance\", \"Variance (σ²):\", min=0.1, max=5.0, value=1.0, step=0.1)\n        ),\n        ui.panel_main(\n            ui.output_plot(\"kernelPlot\")\n        )\n    )\n)\n\ndef server(input, output, session):\n    @output\n    @render.plot\n    def kernelPlot():\n        l = input.length_scale()\n        sigma2 = input.variance()\n        fig = plot_kernel_and_samples(l, sigma2)\n        return fig\n\napp = App(app_ui, server)\n```\n\n### Matérn kernel\n\nThe Matérn kernel is defined as:\n\n$$\nk_{\\text{Matérn}}(x, x') = \\sigma^2 \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left( \\sqrt{2\\nu} \\frac{|x - x'|}{l} \\right)^{\\nu} K_{\\nu} \\left( \\sqrt{2\\nu} \\frac{|x - x'|}{l} \\right)\n$$\n\nwhere $\\nu$ controls the smoothness of the function, $l$ is the length scale, $\\sigma^2$ is the variance, and $K_{\\nu}$ is the modified Bessel function.\nThe former two parameters have the same effect as in the squared exponential kernel, while $\\nu$ controls the smoothness of the function.\nIndeed, we have that the samples generated have smoothness $\\lceil \\nu \\rceil - 1$, and for $\\nu \\to \\infty$, the Matérn kernel converges to the squared exponential kernel, leading to infinitely smooth functions.\n\n```{shinylive-python}\n#| standalone: true\n#| viewerHeight: 475\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\nfrom shiny import App, render, ui\nfrom scipy.special import kv, gamma\nfrom scipy.spatial.distance import cdist\n\n\n# Define the kernel function\ndef matern_kernel(x1, x2, l=1.0, sigma2=1.0, nu=1.5):\n    \"\"\"Computes the Matérn kernel.\"\"\"\n    D = cdist(x1, x2, 'euclidean')\n    const = (2**(1-nu))/gamma(nu)\n    K = const * (np.sqrt(2*nu)*D/l)**nu * kv(nu, np.sqrt(2*nu)*D/l)\n    # Replace NaN values with 1 for x == x'\n    K[np.isnan(K)] = 1\n    K *= sigma2\n    return K\n\n# Define the input space\nX = np.linspace(-4, 4, 100).reshape(-1, 1)\n\ndef plot_kernel_and_samples(l, sigma2, nu):\n    # Compute the kernel matrix\n    K = matern_kernel(X, X, l=l, sigma2=sigma2, nu=nu)\n\n    # Create the plot\n    fig, ax = plt.subplots(2, 1, figsize=(18, 6), sharex=True)\n\n    # Plot the kernel\n    ax[0].plot(X, matern_kernel(X, np.zeros((1, 1)), l=l, sigma2=sigma2, nu=nu))\n    ax[0].set_title(f\"Matern kernel function\")\n\n    # Sample 5 functions from the Gaussian process defined by the kernel\n    mean = np.zeros(100)\n    cov = K\n    samples = multivariate_normal.rvs(mean, cov, 5)\n\n    # Plot the samples\n    for i in range(5):\n        ax[1].plot(X, samples[i])\n    ax[1].set_title(\"Samples from the GP\")\n    ax[1].set_xlabel(\"x\")\n\n    plt.tight_layout()\n    return fig\n\napp_ui = ui.page_fluid(\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_slider(\"length_scale\", \"Length Scale (l):\", min=0.1, max=5.0, value=1.0, step=0.1),\n            ui.input_slider(\"variance\", \"Variance (σ²):\", min=0.1, max=5.0, value=1.0, step=0.1),\n            ui.input_slider(\"smoothness_param\", \"Smoothness param (v):\", min=1.5, max=5.0, value=1.5, step=0.5)\n        ),\n        ui.panel_main(\n            ui.output_plot(\"kernelPlot\")\n        )\n    )\n)\n\ndef server(input, output, session):\n    @output\n    @render.plot\n    def kernelPlot():\n        l = input.length_scale()\n        sigma2 = input.variance()\n        nu = input.smoothness_param()\n        fig = plot_kernel_and_samples(l, sigma2, nu)\n        return fig\n\napp = App(app_ui, server)\n```\n\n## Noisy observations\n\nNoise and measurement error are inevitable in real-world data, which can significantly impact the performance and reliability of predictive models.\nAs a result, it is essential to take noise into account when modeling data with GP.\nWe can represent noisy observations as:\n\n$$\ny = g(x) + \\epsilon\n$$\n\nwhere $\\epsilon$ is a random variable representing the noise.\nUsually, we assume that $\\epsilon \\sim \\mathcal{N}(0, \\sigma_n^2)$, where $\\sigma_n^2$ is the variance of the noise.\nIn this way, it can be easily incorporated into the GP by adding a diagonal noise term to the kernel matrix:\n\n$$\n\\mathbf{K}_n = \\mathbf{K} + \\sigma_n^2 \\mathbf{I},\n$$\n\nwhere $\\mathbf{K}$ is the kernel matrix computed on the training data and $\\mathbf{I}$ is the identity matrix.\n\nBelow is an interactive plot that demonstrates how noise influences the GP model.\nThe plot shows the noisy training data (black points) of the true function $g(x) = \\sin(x)$, the red dashed line.\nThe plot also shows the GP mean prediction (blue line) for the squared exponential and Matérn kernels, along with the 95% confidence intervals.\nFor $\\sigma_n^2 = 0$, the model perfectly interpolates the training data.\nFor higher noise levels, the model becomes less certain about the observations, leading to a non-interpolating behavior, and the confidence intervals widen.\n\n```{shinylive-python}\n#| standalone: true\n#| viewerHeight: 475\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\nfrom shiny import App, render, ui\nfrom scipy.spatial.distance import cdist\nfrom scipy.special import kv, gamma\n\n# Define the kernel functions\ndef exponential_quadratic_kernel(x1, x2, l=1.0, sigma2=1.0):\n    \"\"\"Computes the exponential quadratic kernel (squared exponential kernel).\"\"\"\n    sqdist = np.sum(x1**2, 1).reshape(-1, 1) + np.sum(x2**2, 1) - 2 * np.dot(x1, x2.T)\n    return sigma2 * np.exp(-0.5 / l**2 * sqdist)\n\ndef matern_kernel(x1, x2, l=1.0, sigma2=1.0, nu=1.5):\n    \"\"\"Computes the Matérn kernel.\"\"\"\n    D = cdist(x1, x2, 'euclidean')\n    const = (2**(1-nu))/gamma(nu)\n    K = sigma2 * const * (np.sqrt(2*nu)*D/l)**nu * kv(nu, np.sqrt(2*nu)*D/l)\n    # Replace NaN values with 1 for x == x'\n    K[np.isnan(K)] = 1\n    return K\n\n# Define the input space\nX = np.linspace(-4, 4, 100).reshape(-1, 1)\n\ndef plot_kernels_and_samples(noise_level):\n    # Generate training data with noise\n    X_train = np.array([-3, -2, -1, 1, 3.5]).reshape(-1, 1)\n    y_train = np.sin(X_train) + noise_level * np.random.randn(X_train.shape[0], 1).reshape(-1,1)\n\n    # Compute the kernel matrices for training data\n    K_train_exp = exponential_quadratic_kernel(X_train, X_train) + noise_level**2 * np.eye(len(X_train))\n    K_s_exp = exponential_quadratic_kernel(X_train, X)\n    K_ss_exp = exponential_quadratic_kernel(X, X)\n    \n    K_train_matern = matern_kernel(X_train, X_train) + noise_level**2 * np.eye(len(X_train))\n    K_s_matern = matern_kernel(X_train, X)\n    K_ss_matern = matern_kernel(X, X)\n\n    # Compute the mean and covariance of the posterior distribution for exponential kernel\n    K_train_inv_exp = np.linalg.inv(K_train_exp)\n    mu_s_exp = (K_s_exp.T.dot(K_train_inv_exp).dot(y_train)).reshape(-1)\n    cov_s_exp = K_ss_exp - K_s_exp.T.dot(K_train_inv_exp).dot(K_s_exp)\n\n    # Compute the mean and covariance of the posterior distribution for Matérn kernel\n    K_train_inv_matern = np.linalg.inv(K_train_matern)\n    mu_s_matern = (K_s_matern.T.dot(K_train_inv_matern).dot(y_train)).reshape(-1)\n    cov_s_matern = K_ss_matern - K_s_matern.T.dot(K_train_inv_matern).dot(K_s_matern)\n\n    # Sample 5 functions from the posterior distribution for both kernels\n    samples_exp = multivariate_normal.rvs(mu_s_exp, cov_s_exp, 5)\n    samples_matern = multivariate_normal.rvs(mu_s_matern, cov_s_matern, 5)\n\n    # Create the plot\n    fig, ax = plt.subplots(2, 1, figsize=(18, 6), sharex=True)\n\n    # Plot the training data and GP predictions for exponential kernel\n    ax[0].scatter(X_train, y_train, color='black', zorder=10, label='Noisy observations')\n    ax[0].plot(X, mu_s_exp, color='blue', label='Mean prediction')\n    ax[0].plot(X, np.sin(X), color='r', linestyle='--', label='True function')\n    ax[0].fill_between(X.ravel(), mu_s_exp - 1.96 * np.sqrt(np.diag(cov_s_exp)), mu_s_exp + 1.96 * np.sqrt(np.diag(cov_s_exp)), color=\"blue\", alpha=0.2, label='Confidence interval')\n    # for i in range(5):\n    #     ax[0].plot(X, samples_exp[i], alpha=0.5, linestyle='--')\n    ax[0].set_title(f\"Squared exponential kernel\")\n    # ax[0].legend()\n\n    # Plot the training data and GP predictions for Matérn kernel\n    ax[1].scatter(X_train, y_train, color='black', zorder=10, label='Noisy observations')\n    ax[1].plot(X, mu_s_matern, color=\"blue\", label='Mean prediction')\n    ax[1].plot(X, np.sin(X), color=\"r\", linestyle='--', label='True function')\n    ax[1].fill_between(X.ravel(), mu_s_matern - 1.96 * np.sqrt(np.diag(cov_s_matern)), mu_s_matern + 1.96 * np.sqrt(np.diag(cov_s_matern)), color=\"blue\", alpha=0.2, label='Confidence interval')\n    # for i in range(5):\n    #     ax[1].plot(X, samples_matern[i], alpha=0.5, linestyle='--')\n    ax[1].set_title(f\"Matérn kernel\")\n    # ax[1].legend()\n    ax[1].set_xlabel(\"x\")\n\n    plt.tight_layout()\n    return fig\n\napp_ui = ui.page_fluid(\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_slider(\"noise_level\", \"Noise Level (σ²ₙ):\", min=0.0, max=1.0, value=0.0, step=0.01)\n        ),\n        ui.panel_main(\n            ui.output_plot(\"kernelPlot\")\n        )\n    )\n)\n\ndef server(input, output, session):\n    @output\n    @render.plot\n    def kernelPlot():\n        noise_level = input.noise_level()\n        fig = plot_kernels_and_samples(noise_level)\n        return fig\n\napp = App(app_ui, server)\n```\n\n\n## Code\n\nIn this section, we'll provide a brief overview of how to implement Gaussian processes in Python using the `GPyTorch` library.\nThe code snippet below demonstrates how to define a GP model with a squared exponential kernel and train it on synthetic data.\n\n::: {#d3aca0f8 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport gpytorch\nfrom gpytorch.kernels import RBFKernel, ScaleKernel\nfrom gpytorch.means import ConstantMean\nfrom gpytorch.likelihoods import GaussianLikelihood\nfrom gpytorch.models import ExactGP\nfrom gpytorch.mlls import ExactMarginalLogLikelihood\nfrom gpytorch.distributions import MultivariateNormal\n\n# Define the GP model\nclass GP(ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(GP, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = ConstantMean()\n        self.covar_module = ScaleKernel(RBFKernel())\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return MultivariateNormal(mean_x, covar_x)\n\n# Generate synthetic data\nf = lambda x: torch.sin(x * (2 * np.pi))\ntrain_x = torch.linspace(0, 1, 10)\ntrain_y = f(train_x) + torch.randn(train_x.size()) * 0.1\nlikelihood = GaussianLikelihood()\n\n# Initialize the model and likelihood\nmodel = GP(train_x, train_y, likelihood)\n\n# Training the model\nmodel.train()\nlikelihood.train()\n\n# Use the adam optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\n# \"Loss\" for GPs - the marginal log likelihood\nmll = ExactMarginalLogLikelihood(likelihood, model)\n\n# Training loop\ntraining_iterations = 50\nfor i in range(training_iterations):\n    optimizer.zero_grad()\n    output = model(train_x)\n    loss = -mll(output, train_y)\n    loss.backward()\n    optimizer.step()\n\n# Set the model and likelihood into evaluation mode\nmodel.eval()\nlikelihood.eval()\n\n# Make predictions\ntest_x = torch.linspace(0, 1, 100)\nwith torch.no_grad(), gpytorch.settings.fast_pred_var():\n    observed_pred = likelihood(model(test_x))\n    mean = observed_pred.mean\n    lower, upper = observed_pred.confidence_region()\n\n# Plot the results\nfig, ax = plt.subplots()\nax.scatter(train_x, train_y, color='black', label='Training data')\nax.plot(test_x, f(test_x), 'r--', label='True function')\nax.plot(test_x, mean, 'b', label='Mean prediction')\nax.fill_between(test_x, lower, upper, alpha=0.2, color='blue', label='Confidence interval')\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.legend()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=600 height=429 fig-align='center'}\n:::\n:::\n\n\nThe three main components that need to be defined are the mean function, the kernel function, and the likelihood function (which models the noise in the data).\nObserve that the base kernel is the `RBFKernel`, which corresponds to the squared exponential kernel, and it is wrapped by the `ScaleKernel` to allow for the scaling of the kernel through the variance parameter $\\sigma^2$.\nThe `ExactMarginalLogLikelihood` object is used to compute the marginal log likelihood, which is the negative loss function for training the GP model. Indeed, the GP model parameters are optimized by maximizing the marginal log likelihood of the observed data, which is given by\n\n$$\n\\mathcal{L}(\\theta) = \\log p(Y \\mid X, \\theta)\n    = -\\frac{1}{2} (Y - \\mathbf{m})^\\mathsf{T} \\mathbf{K}_n^{-1} (Y - \\mathbf{m}) - \\frac{1}{2} \\log |\\mathbf{K}_n| - \\frac{N}{2} \\log(2\\pi),\n$$\n\nwhere $\\theta$ are the model parameters, comprising the mean, kernel, and likelihood parameters.\nComputaionally speaking, the inversion of the kernel matrix $\\mathbf{K}_n$ is the most expensive operation, with a complexity of $\\mathcal{O}(n^3)$, where $n$ is the number of training points. Therefore, for very large datasets, approximate inference methods, inducing points, or sparse GPs should be used to reduce the computational burden.\n\nLastly, observe that the `ExactGP` class is the standard GP for Gaussian likelihoods, where the exact marginal log likelihood can be computed in closed form. However, `GPyTorch` also provides different likelihoods, such as student-t likelihoods (which is more stable if outliers are present) and more. In these cases, the must class `ApproximateGP` should be used, which allows for approximate inference methods like variational inference. Regarding the loss function, the `ExactMarginalLogLikelihood` should be replaced by the `VariationalELBO` object, or other appropriate loss functions for approximate inference.\n\n## Applications\n\n### Bayesian hyperparameter tuning\n\nHyperparameter tuning is a critical yet challenging aspect of training neural networks. Finding the optimal combination of hyperparameters, such as learning rate, batch size, number of layers, and units per layer, can significantly enhance a model's performance. Traditional methods like grid search and random search often prove to be inefficient and computationally expensive. This is where Bayesian optimization, powered by GPs, comes into play, offering a smarter approach to hyperparameter tuning.\n\nUnlike exhaustive search methods, Bayesian optimization is more sample-efficient, meaning it can find optimal hyperparameters with fewer iterations. It works by\n\n1. modeling the objective function (e.g., validation loss) as a GP in the hyperparameter space.\n2. using an acquisition function to decide where to sample next. The acquisition function balances exploration (sampling in unexplored regions) and exploitation (sampling in regions with low loss) to guide the search towards the global optimum.\n\n### Surrogate optimization\n\nGPs are also used in surrogate optimization, where the objective function is expensive to evaluate, and we aim to find the global optimum with as few evaluations as possible. By modeling the objective function as a GP, we can make informed decisions about where to sample next, focusing on regions that are likely to contain the global optimum. This can significantly reduce the number of evaluations needed to find the best solution.\n\n### Time series forecasting\n\nGPs are also widely used in time series forecasting due to their flexibility and ability to model complex patterns in the data. By treating time series data as a function of time, Gaussian processes can capture the underlying dynamics and dependencies in the series. They can provide not only point estimates but also probabilistic forecasts, including prediction intervals that quantify uncertainty.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}